{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "import instructor\n",
    "from openai import OpenAI\n",
    "\n",
    "client = instructor.from_openai(OpenAI())\n",
    "\n",
    "dataset_path = \"nl-logql-dataset-classified-metric-with-variables\"\n",
    "dataset = Dataset.load_from_disk(dataset_path)\n",
    "df = dataset.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_colwidth\", 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     False\n",
       "1     False\n",
       "2     False\n",
       "3     False\n",
       "4     False\n",
       "      ...  \n",
       "95    False\n",
       "96    False\n",
       "97    False\n",
       "98    False\n",
       "99    False\n",
       "Name: variables, Length: 100, dtype: bool"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show all rows where variables is None\n",
    "df[\"variables\"].isna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['application', 'id', 'question', 'logql_query', 'query_explanation',\n",
       "       'query_result', 'category', 'log_category_result', 'line_filter',\n",
       "       'label_filter', 'metric_category_result', 'metric_category',\n",
       "       'variables'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"application\":{\"12\":\"openstack\"},\"id\":{\"12\":14},\"question\":{\"12\":\"What is the latest usage status of image 0673dd71-34c5-4fbb-86c4-40623fbe45b4?\"},\"logql_query\":{\"12\":\"(last_over_time({application=\\\"openstack\\\", log_file_type=\\\"nova-compute\\\"}\\n|= \\\"0673dd71-34c5-4fbb-86c4-40623fbe45b4\\\"\\n|= \\\"in use:\\\"\\n| regexp `image (?P<image_id>[^ ]+) at \\\\((?P<image_path>[^)]+)\\\\): in use: on this node (?P<local_use>\\\\d+) local, (?P<other_nodes>\\\\d+) on other nodes sharing this instance storage`\\n| line_format \\\"{{.image_id}} has {{.local_use}}\\\"\\n| unwrap local_use\\n[40d]))\"},\"query_explanation\":{\"12\":\"1\\n{application=\\\"openstack\\\", log_file_type=\\\"nova-compute\\\"}\\nFetch all log lines matching label filters.\\n2\\n<expr> |= `0673dd71-34c5-4fbb-86c4-40623fbe45b4`\\nReturn log lines that contain string 0673dd71-34c5-4fbb-86c4-40623fbe45b4.\\n\\n3\\n<expr> |= `in use:`\\nReturn log lines that contain string in use:.\\n\\n4\\n<expr> | regexp `image (?P<image_id>[^ ]+) at \\\\((?P<image_path>[^)]+)\\\\): in use: on this node (?P<local_use>\\\\d+) local, (?P<other_nodes>\\\\d+) on other nodes sharing this instance storage`\\nThe regexp parser takes a single parameter | regexp \\\"<re>\\\" which is the regular expression using the Golang RE2 syntax. The regular expression must contain a least one named sub-match (e.g (?P<name>re)), each sub-match will extract a different label. The expression matches the structure of a log line. The extracted labels can be used in label filter expressions and used as values for a range aggregation via the unwrap operation.\\n\\n5\\n<expr> | line_format `{{.image_id}} has {{.local_use}}`\\nThis will replace log line using a specified template. The template can refer to stream labels and extracted labels.\\n\\nExample: {{.status_code}} - {{.message}}\\n\\nRead the docs for more.\\n\\n6\\n<expr> | unwrap local_use\\nUse the extracted label local_use as sample values instead of log lines for the subsequent range aggregation.\\n\\n7\\nlast_over_time(<expr> [40d])\\nThe last of all values in the specified interval. Only available in Loki 2.3+. The range vector is set to 40d.\"},\"query_result\":{\"12\":\"0673dd71-34c5-4fbb-86c4-40623fbe45b4 has 1\"},\"category\":{\"12\":\"\"},\"log_category_result\":{\"12\":{\"chain_of_thought\":\"This query utilizes a log stream selector with two label filters: application=\\\"openstack\\\" and log_file_type=\\\"nova-compute\\\". There are also multiple line filters: the first two use the `|=` operator to include log lines containing specific strings, and the third uses a `|regexp` operator to apply a regular expression. The query uses various filters in sequence, thus it falls under the category of multiple line filters and multiple log stream selectors.\",\"label_filter\":\"multiple log stream selectors\",\"line_filter\":\"multiple line filters\"}},\"line_filter\":{\"12\":\"multiple line filters\"},\"label_filter\":{\"12\":\"multiple log stream selectors\"},\"metric_category_result\":{\"12\":{\"categories\":[\"unwrapped_range_aggregation\"],\"chain_of_thought\":\"This query employs `last_over_time`, which is used to select the last value of all points in a specified interval. Since it operates over an `unwrap` expression to utilize a label (`local_use`), it is classified under unwrapped range aggregation. No sign of log range aggregation or built-in range aggregation is evident as the core function `last_over_time` fits directly under unwrapped range aggregation.\"}},\"metric_category\":{\"12\":[\"unwrapped_range_aggregation\"]},\"variables\":{\"12\":[\"image_id\",\"time_in_days\"]}}\n"
     ]
    }
   ],
   "source": [
    "print(df.sample(n=1).to_json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>application</th>\n",
       "      <th>id</th>\n",
       "      <th>question</th>\n",
       "      <th>logql_query</th>\n",
       "      <th>query_explanation</th>\n",
       "      <th>query_result</th>\n",
       "      <th>category</th>\n",
       "      <th>log_category_result</th>\n",
       "      <th>line_filter</th>\n",
       "      <th>label_filter</th>\n",
       "      <th>metric_category_result</th>\n",
       "      <th>metric_category</th>\n",
       "      <th>variables</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>openstack</td>\n",
       "      <td>2</td>\n",
       "      <td>How long did it take to spawn instance 3edec1e...</td>\n",
       "      <td>{application=\"openstack\", log_file_type=\"nova-...</td>\n",
       "      <td>bla</td>\n",
       "      <td>3edec1e4-9678-4a3a-a21b-a145a4ee5e61 took 20.58</td>\n",
       "      <td></td>\n",
       "      <td>{'chain_of_thought': 'Analyzing the query, it ...</td>\n",
       "      <td>multiple line filters</td>\n",
       "      <td>multiple log stream selectors</td>\n",
       "      <td>{'categories': None, 'chain_of_thought': 'The ...</td>\n",
       "      <td>None</td>\n",
       "      <td>[instance_id, time_in_sec]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>openstack</td>\n",
       "      <td>3</td>\n",
       "      <td>What was the total time taken to build instanc...</td>\n",
       "      <td>{application=\"openstack\", log_file_type=\"nova-...</td>\n",
       "      <td>1. {application=\"openstack\", log_file_type=\"no...</td>\n",
       "      <td>21.38</td>\n",
       "      <td></td>\n",
       "      <td>{'chain_of_thought': 'The user query submits u...</td>\n",
       "      <td>multiple line filters</td>\n",
       "      <td>multiple log stream selectors</td>\n",
       "      <td>{'categories': None, 'chain_of_thought': 'This...</td>\n",
       "      <td>None</td>\n",
       "      <td>[instance_id, time_in_sec]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>openstack</td>\n",
       "      <td>4</td>\n",
       "      <td>What was the total time taken to build instanc...</td>\n",
       "      <td>{application=\"openstack\", log_file_type=\"nova-...</td>\n",
       "      <td>1\\n{application=\"openstack\", log_file_type=\"no...</td>\n",
       "      <td>vcpu: 0.00 VCPU used out of 16 VCPU\\ndisk: 0.0...</td>\n",
       "      <td></td>\n",
       "      <td>{'chain_of_thought': 'For the log query in que...</td>\n",
       "      <td>multiple line filters</td>\n",
       "      <td>multiple log stream selectors</td>\n",
       "      <td>{'categories': None, 'chain_of_thought': 'The ...</td>\n",
       "      <td>None</td>\n",
       "      <td>[instance_id, resource]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>openstack</td>\n",
       "      <td>5</td>\n",
       "      <td>What is the vCPU usage for compute node cp-1.s...</td>\n",
       "      <td>max by (node) (\\n  max_over_time(\\n    {applic...</td>\n",
       "      <td>1\\n{application=\"openstack\", log_file_type=\"no...</td>\n",
       "      <td>&lt;graph&gt;\\ngraph with plot of used_vcpus across ...</td>\n",
       "      <td></td>\n",
       "      <td>{'chain_of_thought': 'The query contains three...</td>\n",
       "      <td>multiple line filters</td>\n",
       "      <td>multiple log stream selectors</td>\n",
       "      <td>{'categories': ['unwrapped_range_aggregation',...</td>\n",
       "      <td>[unwrapped_range_aggregation, built_in_range_a...</td>\n",
       "      <td>[compute_node, time_in_days]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>openstack</td>\n",
       "      <td>6</td>\n",
       "      <td>What is the RAM usage for compute node cp-1.sl...</td>\n",
       "      <td>max by (node) (\\n  max_over_time(\\n    {applic...</td>\n",
       "      <td>1\\n{application=\"openstack\", log_file_type=\"no...</td>\n",
       "      <td>&lt;graph&gt;\\\\ngraph with plot of used_vcpus across...</td>\n",
       "      <td></td>\n",
       "      <td>{'chain_of_thought': 'This query has three lab...</td>\n",
       "      <td>multiple line filters</td>\n",
       "      <td>multiple log stream selectors</td>\n",
       "      <td>{'categories': ['built_in_range_aggregation', ...</td>\n",
       "      <td>[built_in_range_aggregation, unwrapped_range_a...</td>\n",
       "      <td>[compute_node, time_in_days]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>openstack</td>\n",
       "      <td>97</td>\n",
       "      <td>What is the total size of all active base files?</td>\n",
       "      <td>sum by (component) (\\n  count_over_time({appli...</td>\n",
       "      <td>1\\n{application=\"openstack\", component=\"nova.v...</td>\n",
       "      <td>12.0k\\n&lt;graph&gt;</td>\n",
       "      <td>Image and File Management</td>\n",
       "      <td>{'chain_of_thought': 'The log query contains t...</td>\n",
       "      <td>single line filter</td>\n",
       "      <td>multiple log stream selectors</td>\n",
       "      <td>{'categories': ['built_in_range_aggregation', ...</td>\n",
       "      <td>[built_in_range_aggregation, log_range_aggrega...</td>\n",
       "      <td>[time_in_hours]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>openstack</td>\n",
       "      <td>98</td>\n",
       "      <td>What is the average response time for GET requ...</td>\n",
       "      <td>avg(\\n  avg_over_time(\\n    {application=\"open...</td>\n",
       "      <td>\\n1. `{application=\"openstack\", log_file_type=...</td>\n",
       "      <td>0.264\\n&lt;graph&gt;</td>\n",
       "      <td>API Performance and Requests</td>\n",
       "      <td>{'chain_of_thought': 'The query specifies two ...</td>\n",
       "      <td>multiple line filters</td>\n",
       "      <td>multiple log stream selectors</td>\n",
       "      <td>{'categories': ['built_in_range_aggregation', ...</td>\n",
       "      <td>[built_in_range_aggregation, unwrapped_range_a...</td>\n",
       "      <td>[http_method, url_endpoint, time_in_hour]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>openstack</td>\n",
       "      <td>99</td>\n",
       "      <td>How many POST requests to /v2/{tenant_id}/os-s...</td>\n",
       "      <td>sum(count_over_time({application=\"openstack\"}\\...</td>\n",
       "      <td>1. `{application=\"openstack\", log_file_type=\"n...</td>\n",
       "      <td>0</td>\n",
       "      <td>API Performance and Requests</td>\n",
       "      <td>{'chain_of_thought': 'The query provided uses ...</td>\n",
       "      <td>multiple line filters</td>\n",
       "      <td>single log stream selector</td>\n",
       "      <td>{'categories': ['log_range_aggregation', 'buil...</td>\n",
       "      <td>[log_range_aggregation, built_in_range_aggrega...</td>\n",
       "      <td>[http_method, url_endpoint, status_code, time_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>openstack</td>\n",
       "      <td>100</td>\n",
       "      <td>What is the 95th percentile response time for ...</td>\n",
       "      <td>quantile_over_time(0.95,\\n  {application=\"open...</td>\n",
       "      <td>1. `{application=\"openstack\", log_file_type=\"n...</td>\n",
       "      <td>0.23</td>\n",
       "      <td>API Performance and Requests</td>\n",
       "      <td>{'chain_of_thought': 'In this query, there are...</td>\n",
       "      <td>multiple line filters</td>\n",
       "      <td>multiple log stream selectors</td>\n",
       "      <td>{'categories': ['unwrapped_range_aggregation']...</td>\n",
       "      <td>[unwrapped_range_aggregation]</td>\n",
       "      <td>[time_in_days, http_method, url_endpoint]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>openstack</td>\n",
       "      <td>101</td>\n",
       "      <td>What is the average time taken to build instan...</td>\n",
       "      <td>avg(\\n  avg_over_time(\\n    {application=\"open...</td>\n",
       "      <td>1. `{application=\"openstack\", component=\"nova....</td>\n",
       "      <td>21.2\\n&lt;graph&gt;</td>\n",
       "      <td>Instance Lifecycle Management</td>\n",
       "      <td>{'chain_of_thought': 'This query makes use of ...</td>\n",
       "      <td>multiple line filters</td>\n",
       "      <td>multiple log stream selectors</td>\n",
       "      <td>{'categories': ['built_in_range_aggregation', ...</td>\n",
       "      <td>[built_in_range_aggregation, unwrapped_range_a...</td>\n",
       "      <td>[time_in_days]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   application   id                                           question  \\\n",
       "0    openstack    2  How long did it take to spawn instance 3edec1e...   \n",
       "1    openstack    3  What was the total time taken to build instanc...   \n",
       "2    openstack    4  What was the total time taken to build instanc...   \n",
       "3    openstack    5  What is the vCPU usage for compute node cp-1.s...   \n",
       "4    openstack    6  What is the RAM usage for compute node cp-1.sl...   \n",
       "..         ...  ...                                                ...   \n",
       "95   openstack   97   What is the total size of all active base files?   \n",
       "96   openstack   98  What is the average response time for GET requ...   \n",
       "97   openstack   99  How many POST requests to /v2/{tenant_id}/os-s...   \n",
       "98   openstack  100  What is the 95th percentile response time for ...   \n",
       "99   openstack  101  What is the average time taken to build instan...   \n",
       "\n",
       "                                          logql_query  \\\n",
       "0   {application=\"openstack\", log_file_type=\"nova-...   \n",
       "1   {application=\"openstack\", log_file_type=\"nova-...   \n",
       "2   {application=\"openstack\", log_file_type=\"nova-...   \n",
       "3   max by (node) (\\n  max_over_time(\\n    {applic...   \n",
       "4   max by (node) (\\n  max_over_time(\\n    {applic...   \n",
       "..                                                ...   \n",
       "95  sum by (component) (\\n  count_over_time({appli...   \n",
       "96  avg(\\n  avg_over_time(\\n    {application=\"open...   \n",
       "97  sum(count_over_time({application=\"openstack\"}\\...   \n",
       "98  quantile_over_time(0.95,\\n  {application=\"open...   \n",
       "99  avg(\\n  avg_over_time(\\n    {application=\"open...   \n",
       "\n",
       "                                    query_explanation  \\\n",
       "0                                                 bla   \n",
       "1   1. {application=\"openstack\", log_file_type=\"no...   \n",
       "2   1\\n{application=\"openstack\", log_file_type=\"no...   \n",
       "3   1\\n{application=\"openstack\", log_file_type=\"no...   \n",
       "4   1\\n{application=\"openstack\", log_file_type=\"no...   \n",
       "..                                                ...   \n",
       "95  1\\n{application=\"openstack\", component=\"nova.v...   \n",
       "96  \\n1. `{application=\"openstack\", log_file_type=...   \n",
       "97  1. `{application=\"openstack\", log_file_type=\"n...   \n",
       "98  1. `{application=\"openstack\", log_file_type=\"n...   \n",
       "99  1. `{application=\"openstack\", component=\"nova....   \n",
       "\n",
       "                                         query_result  \\\n",
       "0     3edec1e4-9678-4a3a-a21b-a145a4ee5e61 took 20.58   \n",
       "1                                               21.38   \n",
       "2   vcpu: 0.00 VCPU used out of 16 VCPU\\ndisk: 0.0...   \n",
       "3   <graph>\\ngraph with plot of used_vcpus across ...   \n",
       "4   <graph>\\\\ngraph with plot of used_vcpus across...   \n",
       "..                                                ...   \n",
       "95                                     12.0k\\n<graph>   \n",
       "96                                     0.264\\n<graph>   \n",
       "97                                                  0   \n",
       "98                                               0.23   \n",
       "99                                      21.2\\n<graph>   \n",
       "\n",
       "                         category  \\\n",
       "0                                   \n",
       "1                                   \n",
       "2                                   \n",
       "3                                   \n",
       "4                                   \n",
       "..                            ...   \n",
       "95      Image and File Management   \n",
       "96   API Performance and Requests   \n",
       "97   API Performance and Requests   \n",
       "98   API Performance and Requests   \n",
       "99  Instance Lifecycle Management   \n",
       "\n",
       "                                  log_category_result            line_filter  \\\n",
       "0   {'chain_of_thought': 'Analyzing the query, it ...  multiple line filters   \n",
       "1   {'chain_of_thought': 'The user query submits u...  multiple line filters   \n",
       "2   {'chain_of_thought': 'For the log query in que...  multiple line filters   \n",
       "3   {'chain_of_thought': 'The query contains three...  multiple line filters   \n",
       "4   {'chain_of_thought': 'This query has three lab...  multiple line filters   \n",
       "..                                                ...                    ...   \n",
       "95  {'chain_of_thought': 'The log query contains t...     single line filter   \n",
       "96  {'chain_of_thought': 'The query specifies two ...  multiple line filters   \n",
       "97  {'chain_of_thought': 'The query provided uses ...  multiple line filters   \n",
       "98  {'chain_of_thought': 'In this query, there are...  multiple line filters   \n",
       "99  {'chain_of_thought': 'This query makes use of ...  multiple line filters   \n",
       "\n",
       "                     label_filter  \\\n",
       "0   multiple log stream selectors   \n",
       "1   multiple log stream selectors   \n",
       "2   multiple log stream selectors   \n",
       "3   multiple log stream selectors   \n",
       "4   multiple log stream selectors   \n",
       "..                            ...   \n",
       "95  multiple log stream selectors   \n",
       "96  multiple log stream selectors   \n",
       "97     single log stream selector   \n",
       "98  multiple log stream selectors   \n",
       "99  multiple log stream selectors   \n",
       "\n",
       "                               metric_category_result  \\\n",
       "0   {'categories': None, 'chain_of_thought': 'The ...   \n",
       "1   {'categories': None, 'chain_of_thought': 'This...   \n",
       "2   {'categories': None, 'chain_of_thought': 'The ...   \n",
       "3   {'categories': ['unwrapped_range_aggregation',...   \n",
       "4   {'categories': ['built_in_range_aggregation', ...   \n",
       "..                                                ...   \n",
       "95  {'categories': ['built_in_range_aggregation', ...   \n",
       "96  {'categories': ['built_in_range_aggregation', ...   \n",
       "97  {'categories': ['log_range_aggregation', 'buil...   \n",
       "98  {'categories': ['unwrapped_range_aggregation']...   \n",
       "99  {'categories': ['built_in_range_aggregation', ...   \n",
       "\n",
       "                                      metric_category  \\\n",
       "0                                                None   \n",
       "1                                                None   \n",
       "2                                                None   \n",
       "3   [unwrapped_range_aggregation, built_in_range_a...   \n",
       "4   [built_in_range_aggregation, unwrapped_range_a...   \n",
       "..                                                ...   \n",
       "95  [built_in_range_aggregation, log_range_aggrega...   \n",
       "96  [built_in_range_aggregation, unwrapped_range_a...   \n",
       "97  [log_range_aggregation, built_in_range_aggrega...   \n",
       "98                      [unwrapped_range_aggregation]   \n",
       "99  [built_in_range_aggregation, unwrapped_range_a...   \n",
       "\n",
       "                                            variables  \n",
       "0                          [instance_id, time_in_sec]  \n",
       "1                          [instance_id, time_in_sec]  \n",
       "2                             [instance_id, resource]  \n",
       "3                        [compute_node, time_in_days]  \n",
       "4                        [compute_node, time_in_days]  \n",
       "..                                                ...  \n",
       "95                                    [time_in_hours]  \n",
       "96          [http_method, url_endpoint, time_in_hour]  \n",
       "97  [http_method, url_endpoint, status_code, time_...  \n",
       "98          [time_in_days, http_method, url_endpoint]  \n",
       "99                                     [time_in_days]  \n",
       "\n",
       "[100 rows x 13 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning\n",
    "\n",
    "- Clean the variables column\n",
    "- Add `row_variables` column for variables pertaining to a row\n",
    "- Add `application_variables` column for each application\n",
    "\n",
    "- For each application, we need to have the log templates stores\n",
    "- For each application we need to have the NL string of the application vars args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "import instructor\n",
    "from openai import OpenAI\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(\".env\")\n",
    "\n",
    "client = instructor.from_openai(OpenAI())\n",
    "\n",
    "dataset_path = \"nl-logql-dataset-classified-metric-with-variables\"\n",
    "dataset = Dataset.load_from_disk(dataset_path)\n",
    "df = dataset.to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read OpenStack Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "openstack_templates_df = pd.read_csv(\n",
    "    \"../logs/OpenStack/OpenStack_full.log_templates.csv\"\n",
    ")\n",
    "openstack_full_log_df = pd.read_csv(\n",
    "    \"../logs/OpenStack/OpenStack_full.log_structured.csv\"\n",
    ")\n",
    "openstack_templates_df = openstack_templates_df.sort_values(\n",
    "    by=\"Occurrences\", ascending=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "openstack_templates = []\n",
    "\n",
    "for index, row in openstack_templates_df.iterrows():\n",
    "    event_id = row[\"EventId\"]\n",
    "    event_template = row[\"EventTemplate\"]\n",
    "    content = openstack_full_log_df[openstack_full_log_df[\"EventId\"] == event_id].iloc[\n",
    "        0\n",
    "    ][\"Content\"]\n",
    "    openstack_templates.append(\n",
    "        {\"EventId\": event_id, \"EventTemplate\": event_template, \"Content\": content}\n",
    "    )\n",
    "\n",
    "openstack_df = pd.DataFrame(openstack_templates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "openstack_application_var = [\"application\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read OpenSSH Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "openssh_templates_df = pd.read_csv(\"../logs/OpenSSH/OpenSSH_full.log_templates.csv\")\n",
    "openssh_full_log_df = pd.read_csv(\"../logs/OpenSSH/OpenSSH_full.log_structured.csv\")\n",
    "openssh_templates_df = openssh_templates_df.sort_values(\n",
    "    by=\"Occurrences\", ascending=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "openssh_templates = []\n",
    "\n",
    "for index, row in openssh_templates_df.iterrows():\n",
    "    event_id = row[\"EventId\"]\n",
    "    event_template = row[\"EventTemplate\"]\n",
    "    content = openssh_full_log_df[openssh_full_log_df[\"EventId\"] == event_id].iloc[0][\n",
    "        \"Content\"\n",
    "    ]\n",
    "    openssh_templates.append(\n",
    "        {\"EventId\": event_id, \"EventTemplate\": event_template, \"Content\": content}\n",
    "    )\n",
    "\n",
    "openssh_df = pd.DataFrame(openssh_templates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "openssh_application_var = [\"application\", \"hostname\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read HDFS Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdfs_templates_df = pd.read_csv(\"../logs/HDFS/HDFS_full.log_templates.csv\")\n",
    "hdfs_full_log_df = pd.read_csv(\"../logs/HDFS/HDFS_full.log_structured.csv\")\n",
    "hdfs_templates_df = hdfs_templates_df.sort_values(by=\"Occurrences\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdfs_templates = []\n",
    "\n",
    "for index, row in hdfs_templates_df.iterrows():\n",
    "    event_id = row[\"EventId\"]\n",
    "    event_template = row[\"EventTemplate\"]\n",
    "    content = hdfs_full_log_df[hdfs_full_log_df[\"EventId\"] == event_id].iloc[0][\n",
    "        \"Content\"\n",
    "    ]\n",
    "    hdfs_templates.append(\n",
    "        {\"EventId\": event_id, \"EventTemplate\": event_template, \"Content\": content}\n",
    "    )\n",
    "hdfs_df = pd.DataFrame(hdfs_templates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdfs_application_var = [\"application\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Common Dict*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Tuple, List\n",
    "from pandas import DataFrame\n",
    "\n",
    "application_dict: Dict[str, Tuple[DataFrame, List]] = {\n",
    "    \"openstack\": (openstack_df, openstack_application_var),\n",
    "    \"hdfs\": (hdfs_df, hdfs_application_var),\n",
    "    \"openssh\": (openssh_df, openssh_application_var),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-process/investigate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "342e145cd83543d88500c79e7cf7ab6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59eba8b168be446fb17be0b09bce1173",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/34 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stats for openssh:\n",
      "Total tokens in openssh rows: 2607\n",
      "Average tokens per openssh row: 76.68\n",
      "Number of openssh rows: 34\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cccbce311914ca49f4ffb1533dc24ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ddb80d535b9471686bc854651f50200",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/38 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stats for openstack:\n",
      "Total tokens in openstack rows: 3753\n",
      "Average tokens per openstack row: 98.76\n",
      "Number of openstack rows: 38\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91c23f22faba4529a5a1b5f549f08470",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e122f58fa0445e8b63f68e997f842aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/28 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stats for hdfs:\n",
      "Total tokens in hdfs rows: 2110\n",
      "Average tokens per hdfs row: 75.36\n",
      "Number of hdfs rows: 28\n",
      "\n",
      "Overall Stats:\n",
      "Total tokens across all applications: 8470\n",
      "Average tokens per row across all applications: 84.70\n",
      "Total number of rows processed: 100\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "\n",
    "def count_tokens(text: str, encoding_name: str = \"cl100k_base\"):\n",
    "    encoding = tiktoken.encoding_for_model(\"gpt-4o\")\n",
    "    # encoding = tiktoken.get_encoding(encoding_name=encoding_name)\n",
    "    return len(encoding.encode(text))\n",
    "\n",
    "\n",
    "def process_token_count(row):\n",
    "    subset_dict = {\n",
    "        \"id\": row.get(\"id\", None),\n",
    "        \"application\": row.get(\"application\", None),\n",
    "        \"logql_query\": row.get(\"logql_query\", None),\n",
    "        \"variables\": row.get(\"variables\", None),\n",
    "        \"query_explanation\": row.get(\"query_explanation\", None),\n",
    "    }\n",
    "    combined_text = f\"{subset_dict['application']} {subset_dict['id']} {subset_dict['logql_query']} {subset_dict['variables']}\"\n",
    "    return {\"token_count\": count_tokens(combined_text)}\n",
    "\n",
    "\n",
    "def process_application(dataset, app_name):\n",
    "    app_dataset = dataset.filter(lambda x: x[\"application\"] == app_name)\n",
    "    app_dataset_tokens = app_dataset.map(process_token_count)\n",
    "    total_tokens = sum(app_dataset_tokens[\"token_count\"])\n",
    "    avg_tokens = (\n",
    "        total_tokens / len(app_dataset_tokens) if len(app_dataset_tokens) > 0 else 0\n",
    "    )\n",
    "\n",
    "    print(f\"\\nStats for {app_name}:\")\n",
    "    print(f\"Total tokens in {app_name} rows: {total_tokens}\")\n",
    "    print(f\"Average tokens per {app_name} row: {avg_tokens:.2f}\")\n",
    "    print(f\"Number of {app_name} rows: {len(app_dataset_tokens)}\")\n",
    "\n",
    "    return app_dataset_tokens\n",
    "\n",
    "\n",
    "# Process each application\n",
    "applications = [\"openssh\", \"openstack\", \"hdfs\"]\n",
    "app_datasets = {}\n",
    "\n",
    "for app in applications:\n",
    "    app_datasets[app] = process_application(dataset, app)\n",
    "\n",
    "# If you want to calculate overall stats\n",
    "all_tokens = sum(sum(app_datasets[app][\"token_count\"]) for app in applications)\n",
    "all_rows = sum(len(app_datasets[app]) for app in applications)\n",
    "overall_avg = all_tokens / all_rows if all_rows > 0 else 0\n",
    "\n",
    "print(\"\\nOverall Stats:\")\n",
    "print(f\"Total tokens across all applications: {all_tokens}\")\n",
    "print(f\"Average tokens per row across all applications: {overall_avg:.2f}\")\n",
    "print(f\"Total number of rows processed: {all_rows}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"\n",
    "You are a senior data analyst, in-charge of creating datasets for training ML models internally. You approach each step of the data curation flow meticulously.\n",
    "\n",
    "You are currently dealing with a dataset for natural language questions to logql queries. LogQL is Grafana Loki's Log Query language for searching through observability and log data.\n",
    "\n",
    "Your current task is to understand existing information about the dataset and different categories in the dataset and add two more columns.\n",
    "\n",
    "First, here's some information about _all_ the rows.\n",
    "1. application\n",
    "   Description: Indicates the software application associated with the log data, in this case \"openstack\".\n",
    "2. id\n",
    "   Description: A unique identifier for each query or question in the dataset.\n",
    "3. question\n",
    "   Description: The human-readable question or task that the LogQL query is designed to answer.\n",
    "4. logql_query\n",
    "   Description: The actual LogQL (Log Query Language) query used to extract information from the logs.\n",
    "5. query_explanation\n",
    "   Description: A detailed explanation of how the LogQL query works and what it does.\n",
    "6. query_result\n",
    "   Description: The output or result obtained from executing the LogQL query.\n",
    "7. category\n",
    "   Description: The general category or type of operation the query pertains to (e.g., \"API Performance and Requests\", \"Instance Lifecycle Management\").\n",
    "8. log_category_result\n",
    "   Description: A JSON-like structure containing analysis of the log query, including a 'chain_of_thought' explanation.\n",
    "9. line_filter\n",
    "   Description: Indicates whether the query uses single or multiple line filters.\n",
    "10. label_filter\n",
    "    Description: Specifies if the query uses single or multiple log stream selectors.\n",
    "11. metric_category_result\n",
    "    Description: A JSON-like structure containing analysis of the metric aspects of the query, including categories and a 'chain_of_thought' explanation.\n",
    "12. metric_category\n",
    "    Description: Lists the specific metric categories identified in the query (e.g., \"unwrapped_range_aggregation\", \"built_in_range_aggregation\").\n",
    "13. variables\n",
    "    Description: A list of variables or parameters used in the query, such as time ranges, instance IDs, or specific resources.\n",
    "\n",
    "You'll be provided with a row of the dataset, templates of the logs the logql query is parsing through and a set of variables for each application that i would want to add.\n",
    "\n",
    "Your task is to add two new columns to this dataset: row_variables and application_variables.\n",
    "\n",
    "row_variables are variables specific to a particular row's logql_query. These are elements in a logql_query that can vary, such as time_in_hours or request_method.\n",
    "\n",
    "application_variables are variables that pertain to an application's logql_query across different rows. These are variables that can vary across all different rows of an application.\n",
    "\n",
    "You'll receive all the rows of a particular application.\n",
    "\n",
    "To complete this task, follow these steps:\n",
    "\n",
    "1. Read each row that will be provided to you. \n",
    "   a. Focus on `logql_query` and `query_explanation` fields to understand the log query and what it's fetching.\n",
    "   b. Focus on `variables` fields to view all the variables the row / application could have. These are human-annotated fields and so are lexically inconsistent but semantically consistent.\n",
    "\n",
    "2. For each unique application in the dataset, create the application_variables column:\n",
    "   a. Identify variables that are mentioned in the application_args_vars.\n",
    "   b. Based on only the information provided, add the variables.\n",
    "   c. Everything else is a row_variable\n",
    "\n",
    "2. For each row, create the row_variables column:\n",
    "   a. Examine the `variables` field to identify variables specific to that query.\n",
    "   b. Based on `logql_query` and `query_explanation`, if needed, re-write variables.\n",
    "   c. Don't add unnecessary variables.\n",
    "\n",
    "4. An application_variable CANNOT be a row_variable. A row_variable CANNOT be an application_variable.\n",
    "\n",
    "5. If an application_variable isn't in the query, still enforce the request and add it!\n",
    "\n",
    "6. You will have access to all the variables, make sure you write the `row_variables` in a consistent manner!\n",
    "\"\"\"\n",
    "# Make sure to include all rows from the original dataset in your output, even if some variables are empty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "USER_PROMPT = \"\"\"<application>\n",
    "{APPLICATION}\n",
    "</application>\n",
    "\n",
    "<dataset_rows>\n",
    "{DATASET_ROW}\n",
    "</dataset_rows>\n",
    "\n",
    "<log_templates>\n",
    "{LOG_TEMPLATES}\n",
    "</log_templates>\n",
    "\n",
    "<application_args_vars>\n",
    "{APPLICATION_ARGS}\n",
    "</application_args_vars>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM Variable Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field, field_validator\n",
    "from typing import List, Dict\n",
    "import json\n",
    "from instructor.utils import disable_pydantic_error_url\n",
    "\n",
    "disable_pydantic_error_url()\n",
    "\n",
    "\n",
    "class Variables(BaseModel):\n",
    "    row_id: int\n",
    "    application: str = Field(\n",
    "        description=\"The application whose logs are being processed.\"\n",
    "    )\n",
    "    chain_of_thought: str = Field(\n",
    "        description=\"Your step-by-step analysis of the instructions and how you would apply them to find the appropriate variables from the dataset row.\"\n",
    "    )\n",
    "    application_variables: List[str] = Field(\n",
    "        description=\"The variables that are pertaining to the entire application.\"\n",
    "    )\n",
    "    row_variables: List[str] = Field(\n",
    "        description=\"The variables pertaining to the dataset row. Meaning, the fields that can be changed in the row without fundamentally changing the query itself.\"\n",
    "    )\n",
    "\n",
    "    @field_validator(\"application\")\n",
    "    @classmethod\n",
    "    def validate_application(cls, v: str) -> str:\n",
    "        allowed_applications = {\"openssh\", \"openstack\", \"hdfs\"}\n",
    "        if v not in allowed_applications:\n",
    "            raise ValueError(f\"application must be one of {allowed_applications}\")\n",
    "        return v\n",
    "\n",
    "    @field_validator(\"application_variables\")\n",
    "    @classmethod\n",
    "    def validate_application_variables(cls, v: List[str], info) -> List[str]:\n",
    "        application = info.data.get(\"application\")\n",
    "        if application == \"openssh\":\n",
    "            expected = [\"hostname\", \"application\"]\n",
    "        elif application in {\"openstack\", \"hdfs\"}:\n",
    "            expected = [\"application\"]\n",
    "        else:\n",
    "            # This shouldn't happen due to the previous validator, but just in case\n",
    "            raise ValueError(\"Invalid application\")\n",
    "\n",
    "        if set(v) != set(expected):\n",
    "            raise ValueError(\n",
    "                f\"For application '{application}', application_variables must be exactly {expected}\"\n",
    "            )\n",
    "        return v\n",
    "\n",
    "\n",
    "def get_var(application: str, row: dict, templates: str, application_vars: List) -> Variables:\n",
    "    return client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        response_model=List[Variables],\n",
    "        max_retries=2,\n",
    "        temperature=0.1,\n",
    "        max_tokens=4096,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": USER_PROMPT.format(\n",
    "                    APPLICATION=application,\n",
    "                    DATASET_ROW=json.dumps(row),\n",
    "                    LOG_TEMPLATES=templates,\n",
    "                    APPLICATION_ARGS=application_vars,\n",
    "                ),\n",
    "            },\n",
    "        ],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\"id\", \"application\", \"logql_query\", \"query_explanation\", \"variables\"]\n",
    "\n",
    "openstack_dataset = dataset.filter(lambda x: x[\"application\"] == \"openstack\")\n",
    "openssh_dataset = dataset.filter(lambda x: x[\"application\"] == \"openssh\")\n",
    "hdfs_dataset = dataset.filter(lambda x: x[\"application\"] == \"hdfs\")\n",
    "\n",
    "openstack_dict = (\n",
    "    openstack_dataset.select_columns(columns).to_pandas().to_json(orient=\"records\")\n",
    ")\n",
    "\n",
    "openssh_dict = (\n",
    "    openssh_dataset.select_columns(columns).to_pandas().to_json(orient=\"records\")\n",
    ")\n",
    "hdfs_dict = hdfs_dataset.select_columns(columns).to_pandas().to_json(orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_row(generated_vars) -> Dict:\n",
    "    # Create a subset dictionary with only the required keys\n",
    "    # subset_dict = {\n",
    "    #     \"id\": row.get(\"id\", None),\n",
    "    #     \"application\": row.get(\"application\", None),\n",
    "    #     \"logql_query\": row.get(\"logql_query\", None),\n",
    "    #     \"query_explanation\": row.get(\"query_explanation\", None),\n",
    "    #     \"variables\": row.get(\"variables\", None),\n",
    "    # }\n",
    "\n",
    "    # Get templates and application_vars from application_dict\n",
    "    # application_name = subset_dict[\"application\"]\n",
    "    templates, application_vars = application_dict.get(application, (None, None))\n",
    "\n",
    "    if templates is not None:\n",
    "        # Convert templates to CSV string\n",
    "        templates_csv = templates.to_csv(index=False)\n",
    "\n",
    "        # Pass the subset_dict to get_var function\n",
    "        new_variables = get_var(application, dataset_rows, )\n",
    "        # get_var(application, dataset_rows)\n",
    "        # print(new_variables)\n",
    "    else:\n",
    "        new_variables = None\n",
    "\n",
    "    # Add new_variables to the original row\n",
    "    row[\"row_variables\"] = new_variables.row_variables\n",
    "    row[\"application_variables\"] = new_variables.application_variables\n",
    "\n",
    "    return row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_variables(example, vars_dict):\n",
    "    row_id = example[\"id\"]  # Assuming your dataset has a 'row_id' column\n",
    "    if row_id in vars_dict:\n",
    "        v = vars_dict[row_id]\n",
    "        example[\"application_variables\"] = v.application_variables\n",
    "        example[\"row_variables\"] = v.row_variables\n",
    "    else:\n",
    "        # Handle the case where row_id is not in variables_dict\n",
    "        example[\"application_variables\"] = []\n",
    "        example[\"row_variables\"] = []\n",
    "    return example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OpenStack**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_msg = USER_PROMPT.format(\n",
    "    APPLICATION=\"openstack\",\n",
    "    DATASET_ROW=openstack_dict,\n",
    "    LOG_TEMPLATES=openstack_templates_df.to_csv(),\n",
    "    APPLICATION_ARGS=openstack_application_var,\n",
    ")\n",
    "openstack_vars = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    response_model=List[Variables],\n",
    "    max_retries=2,\n",
    "    temperature=0.1,\n",
    "    max_tokens=4096,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": user_msg},\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "openstack_variables_dict: Dict[int, Variables] = {v.row_id: v for v in openstack_vars}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sidbin/Main-Quests/sauron/dataset-curation/.venv/lib/python3.12/site-packages/dill/_dill.py:414: PicklingWarning: Cannot locate reference to <class '__main__.Variables'>.\n",
      "  StockPickler.save(self, obj, save_persistent_id)\n",
      "/home/sidbin/Main-Quests/sauron/dataset-curation/.venv/lib/python3.12/site-packages/dill/_dill.py:414: PicklingWarning: Cannot pickle <class '__main__.Variables'>: __main__.Variables has recursive self-references that trigger a RecursionError.\n",
      "  StockPickler.save(self, obj, save_persistent_id)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbf0de47ea7e4c04a981386c1e82c870",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/38 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "openstack_dataset = openstack_dataset.map(\n",
    "    add_variables, fn_kwargs={\"vars_dict\": openstack_variables_dict}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OpenSSH**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_msg = USER_PROMPT.format(\n",
    "    APPLICATION=\"openssh\",\n",
    "    DATASET_ROW=openssh_dict,\n",
    "    LOG_TEMPLATES=openssh_templates_df.to_csv(),\n",
    "    APPLICATION_ARGS=openssh_application_var,\n",
    ")\n",
    "openssh_vars = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    response_model=List[Variables],\n",
    "    max_retries=2,\n",
    "    temperature=0.1,\n",
    "    max_tokens=4096,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": user_msg},\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "openssh_variables_dict: Dict[int, Variables] = {v.row_id: v for v in openssh_vars}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ea4cb0a1efa425eb03ccd08b90a234a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/34 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "openssh_dataset = openssh_dataset.map(\n",
    "    add_variables, fn_kwargs={\"vars_dict\": openssh_variables_dict}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**HDFS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_msg = USER_PROMPT.format(\n",
    "    APPLICATION=\"hdfs\",\n",
    "    DATASET_ROW=hdfs_dict,\n",
    "    LOG_TEMPLATES=hdfs_templates_df.to_csv(),\n",
    "    APPLICATION_ARGS=hdfs_application_var,\n",
    ")\n",
    "hdfs_vars = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    response_model=List[Variables],\n",
    "    max_retries=2,\n",
    "    temperature=0.1,\n",
    "    max_tokens=4096,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": user_msg},\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdfs_variables_dict: Dict[int, Variables] = {v.row_id: v for v in hdfs_vars}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sidbin/Main-Quests/sauron/dataset-curation/.venv/lib/python3.12/site-packages/dill/_dill.py:414: PicklingWarning: Cannot locate reference to <class '__main__.Variables'>.\n",
      "  StockPickler.save(self, obj, save_persistent_id)\n",
      "/home/sidbin/Main-Quests/sauron/dataset-curation/.venv/lib/python3.12/site-packages/dill/_dill.py:414: PicklingWarning: Cannot pickle <class '__main__.Variables'>: __main__.Variables has recursive self-references that trigger a RecursionError.\n",
      "  StockPickler.save(self, obj, save_persistent_id)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7088c1fe6c7469a984f04c7dad8c2e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/28 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "hdfs_dataset = hdfs_dataset.map(\n",
    "    add_variables, fn_kwargs={\"vars_dict\": hdfs_variables_dict}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import concatenate_datasets\n",
    "\n",
    "final_dataset = concatenate_datasets([openstack_dataset, openssh_dataset, hdfs_dataset])\n",
    "# final_dataset.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e76e255c5e4b4ad7b3e7c39b6480f7d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "final_dataset.save_to_disk(\"nl-logql-dataset-00\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>application</th>\n",
       "      <th>id</th>\n",
       "      <th>question</th>\n",
       "      <th>logql_query</th>\n",
       "      <th>query_explanation</th>\n",
       "      <th>query_result</th>\n",
       "      <th>category</th>\n",
       "      <th>log_category_result</th>\n",
       "      <th>line_filter</th>\n",
       "      <th>label_filter</th>\n",
       "      <th>metric_category_result</th>\n",
       "      <th>metric_category</th>\n",
       "      <th>variables</th>\n",
       "      <th>application_variables</th>\n",
       "      <th>row_variables</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>openstack</td>\n",
       "      <td>2</td>\n",
       "      <td>How long did it take to spawn instance 3edec1e4-9678-4a3a-a21b-a145a4ee5e61 on the hypervisor?</td>\n",
       "      <td>{application=\"openstack\", log_file_type=\"nova-compute\", component=\"nova.compute.manager\"} |= \"3edec1e4-9678-4a3a-a21b-a145a4ee5e61\" |= \"Took\" |= \"seconds to spawn the instance on the hypervisor\" | regexp \"\\\\[instance: (?P&lt;instance_id&gt;[^\\\\]]+)\\\\] Took (?P&lt;spawn_time&gt;\\\\d+\\\\.\\\\d+) seconds to spawn the instance on the hypervisor\" | line_format \"{{.instance_id}} took {{.spawn_time}}\"</td>\n",
       "      <td>bla</td>\n",
       "      <td>3edec1e4-9678-4a3a-a21b-a145a4ee5e61 took 20.58</td>\n",
       "      <td></td>\n",
       "      <td>{'chain_of_thought': 'Analyzing the query, it contains three label filters: `application=\"openstack\"`, `log_file_type=\"nova-compute\"`, `component=\"nova.compute.manager\"`. Additionally, there are three line filters (`|= \"3edec1e4-9678-4a3a-a21b-a145a4ee5e61\"`, `|= \"Took\"`, `|= \"seconds to spawn the instance on the hypervisor\"`) followed by a regex operation and a line format operation. The presence of three label filters classifies this under multiple log stream selectors, and the presence of more than one line filter classifies it under multiple line filters.', 'label_filter': 'multiple log stream selectors', 'line_filter': 'multiple line filters'}</td>\n",
       "      <td>multiple line filters</td>\n",
       "      <td>multiple log stream selectors</td>\n",
       "      <td>{'categories': None, 'chain_of_thought': 'The provided query involves log matching and extraction using filters and regexp but does not perform any quantitative aggregations or statistical computations, such as sum, count, or rate on the extracted data. It includes log filters, a regexp pattern for extracting data, and formatting the output using line_format, which suggests manipulation of text rather than numerical computation for metrics. Therefore, there are no metric aggregations present in this query.'}</td>\n",
       "      <td>None</td>\n",
       "      <td>[instance_id, time_in_sec]</td>\n",
       "      <td>[application]</td>\n",
       "      <td>[instance_id, spawn_time]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>openstack</td>\n",
       "      <td>3</td>\n",
       "      <td>What was the total time taken to build instance 3edec1e4-9678-4a3a-a21b-a145a4ee5e61?</td>\n",
       "      <td>{application=\"openstack\", log_file_type=\"nova-compute\"} |= `3edec1e4-9678-4a3a-a21b-a145a4ee5e61` |= `Took` |= `seconds to build instance` | regexp `\\[instance: (?P&lt;instance_id&gt;[^\\]]+)\\] Took (?P&lt;build_time&gt;\\d+\\.\\d+) seconds to build instance` | line_format `{{.build_time}}`</td>\n",
       "      <td>1. {application=\"openstack\", log_file_type=\"nova-compute\"}\\nFetch all log lines matching label filters.\\n2. &lt;expr&gt; |= `3edec1e4-9678-4a3a-a21b-a145a4ee5e61`\\nReturn log lines that contain string 3edec1e4-9678-4a3a-a21b-a145a4ee5e61.\\n\\n3. &lt;expr&gt; |= `Took`\\nReturn log lines that contain string Took.\\n\\n4. &lt;expr&gt; |= `seconds to build instance`\\nReturn log lines that contain string seconds to build instance.\\n\\n5. &lt;expr&gt; | regexp `\\[instance: (?P&lt;instance_id&gt;[^\\]]+)\\] Took (?P&lt;build_time&gt;\\d+\\.\\d+) seconds to build instance`\\nThe regexp parser takes a single parameter | regexp \"&lt;re&gt;\" which is the regular expression using the Golang RE2 syntax. The regular expression must contain a least one named sub-match (e.g (?P&lt;name&gt;re)), each sub-match will extract a different label. The expression matches the structure of a log line. The extracted labels can be used in label filter expressions and used as values for a range aggregation via the unwrap operation.\\n\\n6. &lt;expr&gt; | line_format `{{.buil...</td>\n",
       "      <td>21.38</td>\n",
       "      <td></td>\n",
       "      <td>{'chain_of_thought': 'The user query submits using multiple label filters: `application='openstack'`, `log_file_type='nova-compute'`. There are multiple line filters used sequentially: `|= '3edec1e4-9678-4a3a-a21b-a145a4ee5e61'`, `|= 'Took'`, `|= 'seconds to build instance'`, `| regexp '\\[instance: (?P&lt;instance_id&gt;[^\\]]+)\\d+] Took (?P&lt;build_time&gt;\\d+.\\d+) seconds to build instance'`. By definition, using several different types of line filters suggests it falls under 'multiple line filters'. For labels, using multiple labels as part of the stream selector puts this into the 'multiple log stream selectors' category.', 'label_filter': 'multiple log stream selectors', 'line_filter': 'multiple line filters'}</td>\n",
       "      <td>multiple line filters</td>\n",
       "      <td>multiple log stream selectors</td>\n",
       "      <td>{'categories': None, 'chain_of_thought': 'This LogQL query does not contain any aggregation operators like `sum`, `avg`, `max`, `min`, `count`, etc. It appears to involve parsing and restructuring log lines with `regexp` and `line_format` but does not aggregate these logs into metrics. Therefore, it does not fall into the categories of metric aggregation, whether log range, unwrapped range, or built-in range aggregation.'}</td>\n",
       "      <td>None</td>\n",
       "      <td>[instance_id, time_in_sec]</td>\n",
       "      <td>[application]</td>\n",
       "      <td>[instance_id, build_time]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>openstack</td>\n",
       "      <td>4</td>\n",
       "      <td>What was the total time taken to build instance 3edec1e4-9678-4a3a-a21b-a145a4ee5e61?</td>\n",
       "      <td>{application=\"openstack\", log_file_type=\"nova-compute\"} |= \"3416d0fa-6f0f-41ce-9c0a-59ae9a735da8\" |~ \"Total (memory|disk|vcpu):\" | regexp \"\\\\[instance: (?P&lt;instance_id&gt;[^\\\\]]+)\\\\] Total (?P&lt;resource&gt;\\\\w+): (?P&lt;total&gt;\\\\d+(?:\\\\.\\\\d+)?) (?P&lt;unit&gt;\\\\w+), used: (?P&lt;used&gt;\\\\d+(?:\\\\.\\\\d+)?) \\\\w+\" | line_format \"{{.resource}}: {{.used}} {{.unit}} used out of {{.total}} {{.unit}}\"</td>\n",
       "      <td>1\\n{application=\"openstack\", log_file_type=\"nova-compute\"}\\nFetch all log lines matching label filters.\\n2\\n&lt;expr&gt; |= `3416d0fa-6f0f-41ce-9c0a-59ae9a735da8`\\nReturn log lines that contain string 3416d0fa-6f0f-41ce-9c0a-59ae9a735da8.\\n\\n3\\n&lt;expr&gt; |~ `Total (memory|disk|vcpu):`\\nReturn log lines that match a RE2 regex pattern. Total (memory|disk|vcpu):.\\n\\n4\\n&lt;expr&gt; | regexp `\\[instance: (?P&lt;instance_id&gt;[^\\]]+)\\] Total (?P&lt;resource&gt;\\w+): (?P&lt;total&gt;\\d+(?:\\.\\d+)?) (?P&lt;unit&gt;\\w+), used: (?P&lt;used&gt;\\d+(?:\\.\\d+)?) \\w+`\\nThe regexp parser takes a single parameter | regexp \"&lt;re&gt;\" which is the regular expression using the Golang RE2 syntax. The regular expression must contain a least one named sub-match (e.g (?P&lt;name&gt;re)), each sub-match will extract a different label. The expression matches the structure of a log line. The extracted labels can be used in label filter expressions and used as values for a range aggregation via the unwrap operation.\\n\\n5\\n&lt;expr&gt; | line_format `{{.resource}}: {{.u...</td>\n",
       "      <td>vcpu: 0.00 VCPU used out of 16 VCPU\\ndisk: 0.00 GB used out of 15 GB\\nmemory: 512.00 MB used out of 64172 MB</td>\n",
       "      <td></td>\n",
       "      <td>{'chain_of_thought': 'For the log query in question, there are two label filters specified: `application=\"openstack\"` and `log_file_type=\"nova-compute\"`. This clearly denotes the use of multiple label filters. Concerning line filters, three distinct line filtering expressions are present: `|= \"3416d0fa-6f0f-41ce-9c0a-59ae9a735da8\"`, `|~ \"Total (memory|disk|vcpu):\"`, and `| regexp \"\\[instance: (?P&lt;instance_id&gt;[^\\]]+)\\] Total (?P&lt;resource&gt;\\w+): (?P&lt;total&gt;\\d+(?:\\.\\d+)?) (?P&lt;unit&gt;\\w+), used: (?P&lt;used&gt;\\d+(?:\\.\\d+)?) \\w+\"`. Each of these targets a different element of the log entries, qualifying the query as having multiple line filters.', 'label_filter': 'multiple log stream selectors', 'line_filter': 'multiple line filters'}</td>\n",
       "      <td>multiple line filters</td>\n",
       "      <td>multiple log stream selectors</td>\n",
       "      <td>{'categories': None, 'chain_of_thought': 'The query provided does not include any metric aggregation functions like `sum()`, `count_over_time()`, `rate()`, etc., from LogQL's capabilities. It primarily filters and reformats log lines using operators like `|=`, `|~`, `| regexp`, and `| line_format`. There's no aggregation over time or conversion of log lines into numerical metrics for further statistical operations.'}</td>\n",
       "      <td>None</td>\n",
       "      <td>[instance_id, resource]</td>\n",
       "      <td>[application]</td>\n",
       "      <td>[instance_id, resource, total, unit, used]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>openstack</td>\n",
       "      <td>5</td>\n",
       "      <td>What is the vCPU usage for compute node cp-1.slowvm1.tcloud-pg0.utah.cloudlab.us?</td>\n",
       "      <td>max by (node) (\\n  max_over_time(\\n    {application=\"openstack\", log_file_type=\"nova-compute\", component=\"nova.compute.resource_tracker\"}\\n    |= \"cp-1.slowvm1.tcloud-pg0.utah.cloudlab.us\"\\n    |= \"Final resource view:\"\\n    | regexp \"Final resource view: name=(?P&lt;node&gt;\\\\S+) phys_ram=(?P&lt;total_ram&gt;\\\\d+)MB used_ram=(?P&lt;used_ram&gt;\\\\d+)MB phys_disk=(?P&lt;total_disk&gt;\\\\d+)GB used_disk=(?P&lt;used_disk&gt;\\\\d+)GB total_vcpus=(?P&lt;total_vcpus&gt;\\\\d+) used_vcpus=(?P&lt;used_vcpus&gt;\\\\d+)\"\\n    | label_format\\n        used_vcpus=\"{{.used_vcpus}}\"\\n    | unwrap used_vcpus\\n    [30d]\\n  )\\n)</td>\n",
       "      <td>1\\n{application=\"openstack\", log_file_type=\"nova-compute\", component=\"nova.compute.resource_tracker\"}\\nFetch all log lines matching label filters.\\n2\\n&lt;expr&gt; |= `cp-1.slowvm1.tcloud-pg0.utah.cloudlab.us`\\nReturn log lines that contain string cp-1.slowvm1.tcloud-pg0.utah.cloudlab.us.\\n\\n3\\n&lt;expr&gt; |= `Final resource view:`\\nReturn log lines that contain string Final resource view:.\\n\\n4\\n&lt;expr&gt; | regexp `Final resource view: name=(?P&lt;node&gt;\\S+) phys_ram=(?P&lt;total_ram&gt;\\d+)MB used_ram=(?P&lt;used_ram&gt;\\d+)MB phys_disk=(?P&lt;total_disk&gt;\\d+)GB used_disk=(?P&lt;used_disk&gt;\\d+)GB total_vcpus=(?P&lt;total_vcpus&gt;\\d+) used_vcpus=(?P&lt;used_vcpus&gt;\\d+)`\\nThe regexp parser takes a single parameter | regexp \"&lt;re&gt;\" which is the regular expression using the Golang RE2 syntax. The regular expression must contain a least one named sub-match (e.g (?P&lt;name&gt;re)), each sub-match will extract a different label. The expression matches the structure of a log line. The extracted labels can be used in label filter expression...</td>\n",
       "      <td>&lt;graph&gt;\\ngraph with plot of used_vcpus across different logs.\\nmaximum is 1\\n&lt;/graph&gt;</td>\n",
       "      <td></td>\n",
       "      <td>{'chain_of_thought': 'The query contains three label filters specified within curly braces: `application=\"openstack\"`, `log_file_type=\"nova-compute\"`, and `component=\"nova.compute.resource_tracker\"`. Moreover, there are two line filters used in sequence: `|=` to match the host `cp-1.slowvm1.tcloud-pg0.utah.cloudlab.us` and another `|=` to match the phrase `Final resource view:`. This classifies the query as having multiple label filters and multiple line filters.', 'label_filter': 'multiple log stream selectors', 'line_filter': 'multiple line filters'}</td>\n",
       "      <td>multiple line filters</td>\n",
       "      <td>multiple log stream selectors</td>\n",
       "      <td>{'categories': ['unwrapped_range_aggregation', 'built_in_range_aggregation'], 'chain_of_thought': 'The query uses `max by (node)` which is a built-in aggregation operator that aggregates over the labels. The inner aggregation function `max_over_time` used after unwrapping `used_vcpus` indicates it is an unwrapped range aggregation because it operates on a labeled sample value over a specified time period. Both `max` aggregation operators are involved but in different contexts: one as part of the unwrapped range aggregation, and the other as a built-in range aggregation at the overall level.'}</td>\n",
       "      <td>[unwrapped_range_aggregation, built_in_range_aggregation]</td>\n",
       "      <td>[compute_node, time_in_days]</td>\n",
       "      <td>[application]</td>\n",
       "      <td>[node, total_ram, used_ram, total_disk, used_disk, total_vcpus, used_vcpus]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>openstack</td>\n",
       "      <td>6</td>\n",
       "      <td>What is the RAM usage for compute node cp-1.slowvm1.tcloud-pg0.utah.cloudlab.us?</td>\n",
       "      <td>max by (node) (\\n  max_over_time(\\n    {application=\"openstack\", log_file_type=\"nova-compute\", component=\"nova.compute.resource_tracker\"}\\n    |= \"cp-1.slowvm1.tcloud-pg0.utah.cloudlab.us\"\\n    |= \"Final resource view:\"\\n    | regexp \"Final resource view: name=(?P&lt;node&gt;\\\\S+) phys_ram=(?P&lt;total_ram&gt;\\\\d+)MB used_ram=(?P&lt;used_ram&gt;\\\\d+)MB phys_disk=(?P&lt;total_disk&gt;\\\\d+)GB used_disk=(?P&lt;used_disk&gt;\\\\d+)GB total_vcpus=(?P&lt;total_vcpus&gt;\\\\d+) used_vcpus=(?P&lt;used_vcpus&gt;\\\\d+)\"\\n    | label_format \\n        used_ram_mb=\"{{.used_ram}}\"\\n    | unwrap used_ram_mb\\n    [30d]\\n  )\\n)</td>\n",
       "      <td>1\\n{application=\"openstack\", log_file_type=\"nova-compute\", component=\"nova.compute.resource_tracker\"}\\nFetch all log lines matching label filters.\\n2\\n&lt;expr&gt; |= `cp-1.slowvm1.tcloud-pg0.utah.cloudlab.us`\\nReturn log lines that contain string cp-1.slowvm1.tcloud-pg0.utah.cloudlab.us.\\n\\n3\\n&lt;expr&gt; |= `Final resource view:`\\nReturn log lines that contain string Final resource view:.\\n\\n4\\n&lt;expr&gt; | regexp `Final resource view: name=(?P&lt;node&gt;\\S+) phys_ram=(?P&lt;total_ram&gt;\\d+)MB used_ram=(?P&lt;used_ram&gt;\\d+)MB phys_disk=(?P&lt;total_disk&gt;\\d+)GB used_disk=(?P&lt;used_disk&gt;\\d+)GB total_vcpus=(?P&lt;total_vcpus&gt;\\d+) used_vcpus=(?P&lt;used_vcpus&gt;\\d+)`\\nThe regexp parser takes a single parameter | regexp \"&lt;re&gt;\" which is the regular expression using the Golang RE2 syntax. The regular expression must contain a least one named sub-match (e.g (?P&lt;name&gt;re)), each sub-match will extract a different label. The expression matches the structure of a log line. The extracted labels can be used in label filter expression...</td>\n",
       "      <td>&lt;graph&gt;\\\\ngraph with plot of used_vcpus across different logs. maximum is 2560MB RAM\\n&lt;/graph&gt;\\n</td>\n",
       "      <td></td>\n",
       "      <td>{'chain_of_thought': 'This query has three label filters: `application=\"openstack\"`, `log_file_type=\"nova-compute\"`, `component=\"nova.compute.resource_tracker\"`. Additionally, there are two line filters `|=\"cp-1.slowvm1.tcloud-pg0.utah.cloudlab.us\"` and `|=\"Final resource view:\"` to target specific log entries, followed by a regex match filter `| regexp ...`. Given there are multiple label and line filters employed, both categories indicate multiple filters.', 'label_filter': 'multiple log stream selectors', 'line_filter': 'multiple line filters'}</td>\n",
       "      <td>multiple line filters</td>\n",
       "      <td>multiple log stream selectors</td>\n",
       "      <td>{'categories': ['built_in_range_aggregation', 'unwrapped_range_aggregation'], 'chain_of_thought': 'The query uses two `max` functions which are built-in aggregation operators. The outer `max by (node)` is a built-in aggregation to select maximum values grouped by `node` label. The inner `max_over_time` function operates on an unwrapped range aggregation, using the `unwrap` operator to extract `used_ram_mb` and consider it over a 30 day range. This clearly indicates a combination of both unwrapped and built-in range aggregations.'}</td>\n",
       "      <td>[built_in_range_aggregation, unwrapped_range_aggregation]</td>\n",
       "      <td>[compute_node, time_in_days]</td>\n",
       "      <td>[application]</td>\n",
       "      <td>[node, total_ram, used_ram, total_disk, used_disk, total_vcpus, used_vcpus]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>hdfs</td>\n",
       "      <td>86</td>\n",
       "      <td>What is the total size of blocks added to the blockMap in the last 24 hours?</td>\n",
       "      <td>sum(\\n  sum_over_time(\\n    {component=\"dfs.FSNamesystem\"}\\n    |~ \"BLOCK\\\\* NameSystem\\\\.addStoredBlock: blockMap updated:.*is added to.*size.*\"\\n    | regexp \"BLOCK\\\\* NameSystem\\\\.addStoredBlock: blockMap updated:.*is added to.*size (?P&lt;size&gt;[0-9]+)\"\\n    | unwrap size\\n    [24h]\\n  )\\n)</td>\n",
       "      <td>1. `{component=\"dfs.FSNamesystem\"}`: This selects all logs from the FSNamesystem component, which handles blockMap operations.\\n\\n2. `|~ \"BLOCK\\\\* NameSystem\\\\.addStoredBlock: blockMap updated:.*is added to.*size.*\"`: This line filter matches log lines containing the blockMap update event.\\n\\n3. `| regexp \"BLOCK\\\\* NameSystem\\\\.addStoredBlock: blockMap updated:.*is added to.*size (?P&lt;size&gt;[0-9]+)\"`: This extracts the block size using a regular expression and assigns it to the label \"size\".\\n\\n4. `| unwrap size`: This unwraps the \"size\" label, converting it from a string to a numeric value that can be used in calculations.\\n\\n5. `[24h]`: This specifies the 24-hour time range as requested in the question.\\n\\n6. `sum_over_time(...)`: This sums up all the unwrapped size values over the specified time range.\\n\\n7. `sum(...)`: This calculates the total sum across all instances, giving us the total size of blocks added to the blockMap.\\n\\nThis query efficiently calculates the total size o...</td>\n",
       "      <td>16.1 Tri\\n&lt;graph&gt;</td>\n",
       "      <td>NameNode Operations</td>\n",
       "      <td>{'chain_of_thought': 'This query uses a single label filter: `component=\"dfs.FSNamesystem\"`, to select logs from the specified component. It also includes multiple line filters: the first line filter uses a regular expression to find log lines containing a specific pattern related to stored blocks `|~ \"BLOCK\\* NameSystem\\.addStoredBlock: blockMap updated:.*is added to.*size.*\"`. Following this, another regular expression is applied to extract the size from the log line `| regexp \"BLOCK\\* NameSystem\\.addStoredBlock: blockMap updated:.*is added to.*size (?P&lt;size&gt;[0-9]+)\"`, which further processes the log lines. Thus, the query has a single label filter and multiple line filters.', 'label_filter': 'single log stream selector', 'line_filter': 'multiple line filters'}</td>\n",
       "      <td>multiple line filters</td>\n",
       "      <td>single log stream selector</td>\n",
       "      <td>{'categories': ['built_in_range_aggregation', 'unwrapped_range_aggregation'], 'chain_of_thought': 'In the user query, there is a combination of `sum()` and `sum_over_time()`. The `sum()` function is identified as a built-in aggregation operator, focusing on aggregating metrics based on conditions set within its parameters. The `sum_over_time()` function deals with unwrapped range aggregations where it aggregates values over a specified time period from an unwrapped label. In this case, the label `size` is unwrapped and then aggregated over `24h`. These two aggregation types distinguish the use of built-in and unwrapped range aggregations in the query.'}</td>\n",
       "      <td>[built_in_range_aggregation, unwrapped_range_aggregation]</td>\n",
       "      <td>[time_in_hours]</td>\n",
       "      <td>[application]</td>\n",
       "      <td>[time_in_hours]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>hdfs</td>\n",
       "      <td>87</td>\n",
       "      <td>How many blocks were removed from neededReplications as they didn't belong to any file in the past 12 hours?</td>\n",
       "      <td>count_over_time(\\n  {component=\"dfs.FSNamesystem\"}\\n  |= \"BLOCK* Removing block\" \\n  |= \"from neededReplications as it does not belong to any file\"\\n  [12h]\\n)</td>\n",
       "      <td>1. `{component=\"dfs.FSNamesystem\"}`: This selects all logs from the FSNamesystem component, which handles these operations.\\n\\n2. `|= \"BLOCK* Removing block\"`: This line filter matches log lines containing the beginning of our target message.\\n\\n3. `|= \"from neededReplications as it does not belong to any file\"`: This additional line filter ensures we're matching the exact event we're interested in.\\n\\n4. `[12h]`: This specifies the 12-hour time range as requested in the question.\\n\\n5. `count_over_time(...)`: This counts the occurrences of the matched log lines over the specified time range.\\n\\nThis query efficiently counts the number of blocks removed from neededReplications because they didn't belong to any file in the past 12 hours. The result will be a single value representing the count of such events.\\n</td>\n",
       "      <td>0</td>\n",
       "      <td>NameNode Operations</td>\n",
       "      <td>{'chain_of_thought': 'The query contains a single label filter: `component=\"dfs.FSNamesystem\"`. Additionally, it includes two line filters `|=\"BLOCK* Removing block\"` and `|=\"from neededReplications as it does not belong to any file\"` to specifically match log lines containing these strings. Since there is only one log stream selector and multiple line filters, this query falls into the categories of single log stream selector and multiple line filters.', 'label_filter': 'single log stream selector', 'line_filter': 'multiple line filters'}</td>\n",
       "      <td>multiple line filters</td>\n",
       "      <td>single log stream selector</td>\n",
       "      <td>{'categories': ['log_range_aggregation'], 'chain_of_thought': 'In the query, the aggregation function used is `count_over_time` which is applied over a 12-hour range vector. This function is directly mentioned in the documentation under log range aggregations, where it's used to count the entries within a given range for each log stream. There are no unwrapped range aggregations or built-in aggregation operators directly applied in this query.'}</td>\n",
       "      <td>[log_range_aggregation]</td>\n",
       "      <td>[time_in_hours]</td>\n",
       "      <td>[application]</td>\n",
       "      <td>[time_in_hours]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>hdfs</td>\n",
       "      <td>88</td>\n",
       "      <td>How many blocks took longer than 2 minutes to be received by a DataNode due to SocketTimeoutException in the past 24 hours?\"</td>\n",
       "      <td>sum(\\n  count_over_time(\\n    {component=\"dfs.DataNode$BlockReceiver\"}\\n    |~ \"Exception in receiveBlock for block .* java.net.SocketTimeoutException: .* millis timeout\"\\n    | regexp \"Exception in receiveBlock for block (?P&lt;block_id&gt;blk_[^ ]+) java.net.SocketTimeoutException: (?P&lt;timeout&gt;[0-9]+) millis timeout\"\\n    | timeout &gt; 120000\\n    [24h]\\n  )\\n)</td>\n",
       "      <td>1. `{component=\"dfs.DataNode$BlockReceiver\"}`: This selects all logs from the DataNode component, which handles block receiving operations.\\n\\n2. `|~ \"Exception in receiveBlock for block .* java.net.SocketTimeoutException: .* millis timeout\"`: This line filter matches log lines containing the SocketTimeoutException event for receiving blocks.\\n\\n3. `| regexp \"Exception in receiveBlock for block (?P&lt;block_id&gt;blk_[^ ]+) java.net.SocketTimeoutException: (?P&lt;timeout&gt;[0-9]+) millis timeout\"`: This extracts the block ID and timeout duration using a regular expression and assigns them to the labels \"block_id\" and \"timeout\" respectively.\\n\\n4. `| timeout &gt; 120000`: This filters for events where the timeout is greater than 120000 milliseconds (2 minutes).\\n\\n5. `[24h]`: This specifies the 24-hour time range as requested in the question.\\n\\n6. `count_over_time(...)`: This counts the occurrences of events that match our criteria over the specified time range.\\n\\n7. `sum(...)`: This sums up al...</td>\n",
       "      <td>0</td>\n",
       "      <td>Performance Issues</td>\n",
       "      <td>{'chain_of_thought': 'The given LogQL query has a single label filter, which is {component=\"dfs.DataNode$BlockReceiver\"}. This selector identifies the specific component generating the logs. There are multiple line filters used in the query: one regular expression filter looking for a pattern with a 'SocketTimeoutException' and another regular expression extracting specific details about the block ID and timeout value using named groups. Additionally, there is a filter checking if the timeout exceeds a certain threshold. These elements indicate the use of multiple line filters since more than one type of line filtering is being applied.', 'label_filter': 'single log stream selector', 'line_filter': 'multiple line filters'}</td>\n",
       "      <td>multiple line filters</td>\n",
       "      <td>single log stream selector</td>\n",
       "      <td>{'categories': ['log_range_aggregation', 'built_in_range_aggregation'], 'chain_of_thought': 'In this query, two primary operations are observed. The `sum()` function is a built-in aggregation operator that aggregates values across a dataset. The other function, `count_over_time()`, which is used within the `sum()`, is a log range aggregation as it applies a time-based count over log entries. The `count_over_time()` is applied over a 24-hour window, which involves counting entries that match the log pattern and conditions specified.'}</td>\n",
       "      <td>[log_range_aggregation, built_in_range_aggregation]</td>\n",
       "      <td>[time_in_minutes, time_in_hours]</td>\n",
       "      <td>[application]</td>\n",
       "      <td>[timeout, time_in_hours]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>hdfs</td>\n",
       "      <td>89</td>\n",
       "      <td>How many times did the PendingReplicationMonitor time out for blocks in the past 12 hours?</td>\n",
       "      <td>sum(\\n  count_over_time(\\n    {application=\"hdfs\"}\\n    |~ \"PendingReplicationMonitor timed out block .*\"\\n    [12h]\\n  )\\n)</td>\n",
       "      <td>Explanation of the query:\\n\\n1. `{application=\"hdfs\"}`: This selects all logs from the HDFS application, as we don't have a specific component for PendingReplicationMonitor.\\n\\n2. `|~ \"PendingReplicationMonitor timed out block .*\"`: This line filter matches log lines containing the PendingReplicationMonitor timeout event.\\n\\n3. `[12h]`: This specifies the 12-hour time range as requested in the question.\\n\\n4. `count_over_time(...)`: This counts the occurrences of the matched log lines over the specified time range.\\n\\n5. `sum(...)`: This sums up all the counts, giving us the total number of times the PendingReplicationMonitor timed out for blocks in the past 12 hours.\\n\\nThis query efficiently counts the number of PendingReplicationMonitor timeout events across all HDFS components in the last 12 hours. The result will be a single value representing the total count of these timeout events.\\n</td>\n",
       "      <td>2</td>\n",
       "      <td>Performance Issues</td>\n",
       "      <td>{'chain_of_thought': 'This query includes a single label filter: `application=\"hdfs\"`. Additionally, it contains a single line filter `|~ \"PendingReplicationMonitor timed out block .*\"` used to match logs with a specific pattern. There are no multiple filters used.', 'label_filter': 'single log stream selector', 'line_filter': 'single line filter'}</td>\n",
       "      <td>single line filter</td>\n",
       "      <td>single log stream selector</td>\n",
       "      <td>{'categories': ['log_range_aggregation', 'built_in_range_aggregation'], 'chain_of_thought': 'The query uses the `sum()` function as well as the `count_over_time()` function over a logging range of 12 hours specified. According to the documentation, `count_over_time` is categorized as a log range aggregation as it counts log entries over a specified time range. `sum()` is a built-in aggregation operator, used here to aggregate the counts over all labels.'}</td>\n",
       "      <td>[log_range_aggregation, built_in_range_aggregation]</td>\n",
       "      <td>[time_in_hours]</td>\n",
       "      <td>[application]</td>\n",
       "      <td>[time_in_hours]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>hdfs</td>\n",
       "      <td>90</td>\n",
       "      <td>What is the average time taken for a block to be transmitted between DataNodes in the last hour?</td>\n",
       "      <td>(\\n  sum(rate({application=\"hdfs\"} |~ \"Transmitted block\" [1h])) /\\n  sum(rate({application=\"hdfs\"} |~ \"Starting thread to transfer block\" [1h]))\\n) * 3600</td>\n",
       "      <td>Explanation of the query:\\n\\n1. `{application=\"hdfs\"}`: This selects all logs from  HDFS application.\\n\\n2. `|~ \"Transmitted block\"` and `|~ \"Starting thread to transfer block\"`: These line filters match log lines containing the end and start of block transfer events, respectively.\\n\\n3. `[1h]`: This specifies the 1-hour time range as requested in the question.\\n\\n4. `rate(... [1h])`: This calculates the per-second rate of occurrences for each event over the last hour.\\n\\n5. `sum(...)`: This sums the rates across all DataNodes.\\n\\n6. The division `(...) / (...)` gives us the average time between start and end events.\\n\\n7. `* 3600`: This converts the result from seconds to hours.\\n\\nThis query approximates the average time taken for a block to be transmitted between DataNodes in the last hour. It does this by calculating the ratio of completed transmissions to started transmissions and then converting this to an average time in seconds.\\n\\nNote that this method assumes that the rat...</td>\n",
       "      <td>38k\\n&lt;graph&gt;</td>\n",
       "      <td>Replication and Data Transfer</td>\n",
       "      <td>{'chain_of_thought': 'The query has a single label filter: `application=\"hdfs\"`. The query is checking rates over log lines that match two different line filters: `|~ \"Transmitted block\"` and `|~ \"Starting thread to transfer block\"`. Since there are two different line filters used in separate sub-queries, this qualifies as multiple line filters. Therefore, the labels are single, and the line filters are multiple.', 'label_filter': 'single log stream selector', 'line_filter': 'multiple line filters'}</td>\n",
       "      <td>multiple line filters</td>\n",
       "      <td>single log stream selector</td>\n",
       "      <td>{'categories': ['log_range_aggregation', 'built_in_range_aggregation'], 'chain_of_thought': 'In this query, `sum()` and `rate()` are used. From the LogQL documentation, `rate()` functions as a log range aggregation, calculating the rate of logs over a specified time period. The `sum()` function is a built-in aggregation operator used here to sum up the rates calculated. The entire expression calculates a rate over an hour and uses built-in aggregation operators to sum these rates. These sums are then combined in a mathematical expression.'}</td>\n",
       "      <td>[log_range_aggregation, built_in_range_aggregation]</td>\n",
       "      <td>[time_in_hours]</td>\n",
       "      <td>[application]</td>\n",
       "      <td>[time_in_hours]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   application  id  \\\n",
       "0    openstack   2   \n",
       "1    openstack   3   \n",
       "2    openstack   4   \n",
       "3    openstack   5   \n",
       "4    openstack   6   \n",
       "..         ...  ..   \n",
       "95        hdfs  86   \n",
       "96        hdfs  87   \n",
       "97        hdfs  88   \n",
       "98        hdfs  89   \n",
       "99        hdfs  90   \n",
       "\n",
       "                                                                                                                        question  \\\n",
       "0                                 How long did it take to spawn instance 3edec1e4-9678-4a3a-a21b-a145a4ee5e61 on the hypervisor?   \n",
       "1                                          What was the total time taken to build instance 3edec1e4-9678-4a3a-a21b-a145a4ee5e61?   \n",
       "2                                          What was the total time taken to build instance 3edec1e4-9678-4a3a-a21b-a145a4ee5e61?   \n",
       "3                                              What is the vCPU usage for compute node cp-1.slowvm1.tcloud-pg0.utah.cloudlab.us?   \n",
       "4                                               What is the RAM usage for compute node cp-1.slowvm1.tcloud-pg0.utah.cloudlab.us?   \n",
       "..                                                                                                                           ...   \n",
       "95                                                  What is the total size of blocks added to the blockMap in the last 24 hours?   \n",
       "96                  How many blocks were removed from neededReplications as they didn't belong to any file in the past 12 hours?   \n",
       "97  How many blocks took longer than 2 minutes to be received by a DataNode due to SocketTimeoutException in the past 24 hours?\"   \n",
       "98                                    How many times did the PendingReplicationMonitor time out for blocks in the past 12 hours?   \n",
       "99                              What is the average time taken for a block to be transmitted between DataNodes in the last hour?   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    logql_query  \\\n",
       "0                                                                                                                                                                                                 {application=\"openstack\", log_file_type=\"nova-compute\", component=\"nova.compute.manager\"} |= \"3edec1e4-9678-4a3a-a21b-a145a4ee5e61\" |= \"Took\" |= \"seconds to spawn the instance on the hypervisor\" | regexp \"\\\\[instance: (?P<instance_id>[^\\\\]]+)\\\\] Took (?P<spawn_time>\\\\d+\\\\.\\\\d+) seconds to spawn the instance on the hypervisor\" | line_format \"{{.instance_id}} took {{.spawn_time}}\"   \n",
       "1                                                                                                                                                                                                                                                                                                           {application=\"openstack\", log_file_type=\"nova-compute\"} |= `3edec1e4-9678-4a3a-a21b-a145a4ee5e61` |= `Took` |= `seconds to build instance` | regexp `\\[instance: (?P<instance_id>[^\\]]+)\\] Took (?P<build_time>\\d+\\.\\d+) seconds to build instance` | line_format `{{.build_time}}`   \n",
       "2                                                                                                                                                                                                          {application=\"openstack\", log_file_type=\"nova-compute\"} |= \"3416d0fa-6f0f-41ce-9c0a-59ae9a735da8\" |~ \"Total (memory|disk|vcpu):\" | regexp \"\\\\[instance: (?P<instance_id>[^\\\\]]+)\\\\] Total (?P<resource>\\\\w+): (?P<total>\\\\d+(?:\\\\.\\\\d+)?) (?P<unit>\\\\w+), used: (?P<used>\\\\d+(?:\\\\.\\\\d+)?) \\\\w+\" | line_format \"{{.resource}}: {{.used}} {{.unit}} used out of {{.total}} {{.unit}}\"   \n",
       "3    max by (node) (\\n  max_over_time(\\n    {application=\"openstack\", log_file_type=\"nova-compute\", component=\"nova.compute.resource_tracker\"}\\n    |= \"cp-1.slowvm1.tcloud-pg0.utah.cloudlab.us\"\\n    |= \"Final resource view:\"\\n    | regexp \"Final resource view: name=(?P<node>\\\\S+) phys_ram=(?P<total_ram>\\\\d+)MB used_ram=(?P<used_ram>\\\\d+)MB phys_disk=(?P<total_disk>\\\\d+)GB used_disk=(?P<used_disk>\\\\d+)GB total_vcpus=(?P<total_vcpus>\\\\d+) used_vcpus=(?P<used_vcpus>\\\\d+)\"\\n    | label_format\\n        used_vcpus=\"{{.used_vcpus}}\"\\n    | unwrap used_vcpus\\n    [30d]\\n  )\\n)   \n",
       "4   max by (node) (\\n  max_over_time(\\n    {application=\"openstack\", log_file_type=\"nova-compute\", component=\"nova.compute.resource_tracker\"}\\n    |= \"cp-1.slowvm1.tcloud-pg0.utah.cloudlab.us\"\\n    |= \"Final resource view:\"\\n    | regexp \"Final resource view: name=(?P<node>\\\\S+) phys_ram=(?P<total_ram>\\\\d+)MB used_ram=(?P<used_ram>\\\\d+)MB phys_disk=(?P<total_disk>\\\\d+)GB used_disk=(?P<used_disk>\\\\d+)GB total_vcpus=(?P<total_vcpus>\\\\d+) used_vcpus=(?P<used_vcpus>\\\\d+)\"\\n    | label_format \\n        used_ram_mb=\"{{.used_ram}}\"\\n    | unwrap used_ram_mb\\n    [30d]\\n  )\\n)   \n",
       "..                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          ...   \n",
       "95                                                                                                                                                                                                                                                                                          sum(\\n  sum_over_time(\\n    {component=\"dfs.FSNamesystem\"}\\n    |~ \"BLOCK\\\\* NameSystem\\\\.addStoredBlock: blockMap updated:.*is added to.*size.*\"\\n    | regexp \"BLOCK\\\\* NameSystem\\\\.addStoredBlock: blockMap updated:.*is added to.*size (?P<size>[0-9]+)\"\\n    | unwrap size\\n    [24h]\\n  )\\n)   \n",
       "96                                                                                                                                                                                                                                                                                                                                                                                                                              count_over_time(\\n  {component=\"dfs.FSNamesystem\"}\\n  |= \"BLOCK* Removing block\" \\n  |= \"from neededReplications as it does not belong to any file\"\\n  [12h]\\n)   \n",
       "97                                                                                                                                                                                                                        sum(\\n  count_over_time(\\n    {component=\"dfs.DataNode$BlockReceiver\"}\\n    |~ \"Exception in receiveBlock for block .* java.net.SocketTimeoutException: .* millis timeout\"\\n    | regexp \"Exception in receiveBlock for block (?P<block_id>blk_[^ ]+) java.net.SocketTimeoutException: (?P<timeout>[0-9]+) millis timeout\"\\n    | timeout > 120000\\n    [24h]\\n  )\\n)   \n",
       "98                                                                                                                                                                                                                                                                                                                                                                                                                                                                 sum(\\n  count_over_time(\\n    {application=\"hdfs\"}\\n    |~ \"PendingReplicationMonitor timed out block .*\"\\n    [12h]\\n  )\\n)   \n",
       "99                                                                                                                                                                                                                                                                                                                                                                                                                                  (\\n  sum(rate({application=\"hdfs\"} |~ \"Transmitted block\" [1h])) /\\n  sum(rate({application=\"hdfs\"} |~ \"Starting thread to transfer block\" [1h]))\\n) * 3600   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          query_explanation  \\\n",
       "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       bla   \n",
       "1   1. {application=\"openstack\", log_file_type=\"nova-compute\"}\\nFetch all log lines matching label filters.\\n2. <expr> |= `3edec1e4-9678-4a3a-a21b-a145a4ee5e61`\\nReturn log lines that contain string 3edec1e4-9678-4a3a-a21b-a145a4ee5e61.\\n\\n3. <expr> |= `Took`\\nReturn log lines that contain string Took.\\n\\n4. <expr> |= `seconds to build instance`\\nReturn log lines that contain string seconds to build instance.\\n\\n5. <expr> | regexp `\\[instance: (?P<instance_id>[^\\]]+)\\] Took (?P<build_time>\\d+\\.\\d+) seconds to build instance`\\nThe regexp parser takes a single parameter | regexp \"<re>\" which is the regular expression using the Golang RE2 syntax. The regular expression must contain a least one named sub-match (e.g (?P<name>re)), each sub-match will extract a different label. The expression matches the structure of a log line. The extracted labels can be used in label filter expressions and used as values for a range aggregation via the unwrap operation.\\n\\n6. <expr> | line_format `{{.buil...   \n",
       "2   1\\n{application=\"openstack\", log_file_type=\"nova-compute\"}\\nFetch all log lines matching label filters.\\n2\\n<expr> |= `3416d0fa-6f0f-41ce-9c0a-59ae9a735da8`\\nReturn log lines that contain string 3416d0fa-6f0f-41ce-9c0a-59ae9a735da8.\\n\\n3\\n<expr> |~ `Total (memory|disk|vcpu):`\\nReturn log lines that match a RE2 regex pattern. Total (memory|disk|vcpu):.\\n\\n4\\n<expr> | regexp `\\[instance: (?P<instance_id>[^\\]]+)\\] Total (?P<resource>\\w+): (?P<total>\\d+(?:\\.\\d+)?) (?P<unit>\\w+), used: (?P<used>\\d+(?:\\.\\d+)?) \\w+`\\nThe regexp parser takes a single parameter | regexp \"<re>\" which is the regular expression using the Golang RE2 syntax. The regular expression must contain a least one named sub-match (e.g (?P<name>re)), each sub-match will extract a different label. The expression matches the structure of a log line. The extracted labels can be used in label filter expressions and used as values for a range aggregation via the unwrap operation.\\n\\n5\\n<expr> | line_format `{{.resource}}: {{.u...   \n",
       "3   1\\n{application=\"openstack\", log_file_type=\"nova-compute\", component=\"nova.compute.resource_tracker\"}\\nFetch all log lines matching label filters.\\n2\\n<expr> |= `cp-1.slowvm1.tcloud-pg0.utah.cloudlab.us`\\nReturn log lines that contain string cp-1.slowvm1.tcloud-pg0.utah.cloudlab.us.\\n\\n3\\n<expr> |= `Final resource view:`\\nReturn log lines that contain string Final resource view:.\\n\\n4\\n<expr> | regexp `Final resource view: name=(?P<node>\\S+) phys_ram=(?P<total_ram>\\d+)MB used_ram=(?P<used_ram>\\d+)MB phys_disk=(?P<total_disk>\\d+)GB used_disk=(?P<used_disk>\\d+)GB total_vcpus=(?P<total_vcpus>\\d+) used_vcpus=(?P<used_vcpus>\\d+)`\\nThe regexp parser takes a single parameter | regexp \"<re>\" which is the regular expression using the Golang RE2 syntax. The regular expression must contain a least one named sub-match (e.g (?P<name>re)), each sub-match will extract a different label. The expression matches the structure of a log line. The extracted labels can be used in label filter expression...   \n",
       "4   1\\n{application=\"openstack\", log_file_type=\"nova-compute\", component=\"nova.compute.resource_tracker\"}\\nFetch all log lines matching label filters.\\n2\\n<expr> |= `cp-1.slowvm1.tcloud-pg0.utah.cloudlab.us`\\nReturn log lines that contain string cp-1.slowvm1.tcloud-pg0.utah.cloudlab.us.\\n\\n3\\n<expr> |= `Final resource view:`\\nReturn log lines that contain string Final resource view:.\\n\\n4\\n<expr> | regexp `Final resource view: name=(?P<node>\\S+) phys_ram=(?P<total_ram>\\d+)MB used_ram=(?P<used_ram>\\d+)MB phys_disk=(?P<total_disk>\\d+)GB used_disk=(?P<used_disk>\\d+)GB total_vcpus=(?P<total_vcpus>\\d+) used_vcpus=(?P<used_vcpus>\\d+)`\\nThe regexp parser takes a single parameter | regexp \"<re>\" which is the regular expression using the Golang RE2 syntax. The regular expression must contain a least one named sub-match (e.g (?P<name>re)), each sub-match will extract a different label. The expression matches the structure of a log line. The extracted labels can be used in label filter expression...   \n",
       "..                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      ...   \n",
       "95  1. `{component=\"dfs.FSNamesystem\"}`: This selects all logs from the FSNamesystem component, which handles blockMap operations.\\n\\n2. `|~ \"BLOCK\\\\* NameSystem\\\\.addStoredBlock: blockMap updated:.*is added to.*size.*\"`: This line filter matches log lines containing the blockMap update event.\\n\\n3. `| regexp \"BLOCK\\\\* NameSystem\\\\.addStoredBlock: blockMap updated:.*is added to.*size (?P<size>[0-9]+)\"`: This extracts the block size using a regular expression and assigns it to the label \"size\".\\n\\n4. `| unwrap size`: This unwraps the \"size\" label, converting it from a string to a numeric value that can be used in calculations.\\n\\n5. `[24h]`: This specifies the 24-hour time range as requested in the question.\\n\\n6. `sum_over_time(...)`: This sums up all the unwrapped size values over the specified time range.\\n\\n7. `sum(...)`: This calculates the total sum across all instances, giving us the total size of blocks added to the blockMap.\\n\\nThis query efficiently calculates the total size o...   \n",
       "96                                                                                                                                                                                    1. `{component=\"dfs.FSNamesystem\"}`: This selects all logs from the FSNamesystem component, which handles these operations.\\n\\n2. `|= \"BLOCK* Removing block\"`: This line filter matches log lines containing the beginning of our target message.\\n\\n3. `|= \"from neededReplications as it does not belong to any file\"`: This additional line filter ensures we're matching the exact event we're interested in.\\n\\n4. `[12h]`: This specifies the 12-hour time range as requested in the question.\\n\\n5. `count_over_time(...)`: This counts the occurrences of the matched log lines over the specified time range.\\n\\nThis query efficiently counts the number of blocks removed from neededReplications because they didn't belong to any file in the past 12 hours. The result will be a single value representing the count of such events.\\n   \n",
       "97  1. `{component=\"dfs.DataNode$BlockReceiver\"}`: This selects all logs from the DataNode component, which handles block receiving operations.\\n\\n2. `|~ \"Exception in receiveBlock for block .* java.net.SocketTimeoutException: .* millis timeout\"`: This line filter matches log lines containing the SocketTimeoutException event for receiving blocks.\\n\\n3. `| regexp \"Exception in receiveBlock for block (?P<block_id>blk_[^ ]+) java.net.SocketTimeoutException: (?P<timeout>[0-9]+) millis timeout\"`: This extracts the block ID and timeout duration using a regular expression and assigns them to the labels \"block_id\" and \"timeout\" respectively.\\n\\n4. `| timeout > 120000`: This filters for events where the timeout is greater than 120000 milliseconds (2 minutes).\\n\\n5. `[24h]`: This specifies the 24-hour time range as requested in the question.\\n\\n6. `count_over_time(...)`: This counts the occurrences of events that match our criteria over the specified time range.\\n\\n7. `sum(...)`: This sums up al...   \n",
       "98                                                                                                  Explanation of the query:\\n\\n1. `{application=\"hdfs\"}`: This selects all logs from the HDFS application, as we don't have a specific component for PendingReplicationMonitor.\\n\\n2. `|~ \"PendingReplicationMonitor timed out block .*\"`: This line filter matches log lines containing the PendingReplicationMonitor timeout event.\\n\\n3. `[12h]`: This specifies the 12-hour time range as requested in the question.\\n\\n4. `count_over_time(...)`: This counts the occurrences of the matched log lines over the specified time range.\\n\\n5. `sum(...)`: This sums up all the counts, giving us the total number of times the PendingReplicationMonitor timed out for blocks in the past 12 hours.\\n\\nThis query efficiently counts the number of PendingReplicationMonitor timeout events across all HDFS components in the last 12 hours. The result will be a single value representing the total count of these timeout events.\\n   \n",
       "99  Explanation of the query:\\n\\n1. `{application=\"hdfs\"}`: This selects all logs from  HDFS application.\\n\\n2. `|~ \"Transmitted block\"` and `|~ \"Starting thread to transfer block\"`: These line filters match log lines containing the end and start of block transfer events, respectively.\\n\\n3. `[1h]`: This specifies the 1-hour time range as requested in the question.\\n\\n4. `rate(... [1h])`: This calculates the per-second rate of occurrences for each event over the last hour.\\n\\n5. `sum(...)`: This sums the rates across all DataNodes.\\n\\n6. The division `(...) / (...)` gives us the average time between start and end events.\\n\\n7. `* 3600`: This converts the result from seconds to hours.\\n\\nThis query approximates the average time taken for a block to be transmitted between DataNodes in the last hour. It does this by calculating the ratio of completed transmissions to started transmissions and then converting this to an average time in seconds.\\n\\nNote that this method assumes that the rat...   \n",
       "\n",
       "                                                                                                    query_result  \\\n",
       "0                                                                3edec1e4-9678-4a3a-a21b-a145a4ee5e61 took 20.58   \n",
       "1                                                                                                          21.38   \n",
       "2   vcpu: 0.00 VCPU used out of 16 VCPU\\ndisk: 0.00 GB used out of 15 GB\\nmemory: 512.00 MB used out of 64172 MB   \n",
       "3                          <graph>\\ngraph with plot of used_vcpus across different logs.\\nmaximum is 1\\n</graph>   \n",
       "4               <graph>\\\\ngraph with plot of used_vcpus across different logs. maximum is 2560MB RAM\\n</graph>\\n   \n",
       "..                                                                                                           ...   \n",
       "95                                                                                             16.1 Tri\\n<graph>   \n",
       "96                                                                                                             0   \n",
       "97                                                                                                             0   \n",
       "98                                                                                                             2   \n",
       "99                                                                                                  38k\\n<graph>   \n",
       "\n",
       "                         category  \\\n",
       "0                                   \n",
       "1                                   \n",
       "2                                   \n",
       "3                                   \n",
       "4                                   \n",
       "..                            ...   \n",
       "95            NameNode Operations   \n",
       "96            NameNode Operations   \n",
       "97             Performance Issues   \n",
       "98             Performance Issues   \n",
       "99  Replication and Data Transfer   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      log_category_result  \\\n",
       "0                                                                                                                        {'chain_of_thought': 'Analyzing the query, it contains three label filters: `application=\"openstack\"`, `log_file_type=\"nova-compute\"`, `component=\"nova.compute.manager\"`. Additionally, there are three line filters (`|= \"3edec1e4-9678-4a3a-a21b-a145a4ee5e61\"`, `|= \"Took\"`, `|= \"seconds to spawn the instance on the hypervisor\"`) followed by a regex operation and a line format operation. The presence of three label filters classifies this under multiple log stream selectors, and the presence of more than one line filter classifies it under multiple line filters.', 'label_filter': 'multiple log stream selectors', 'line_filter': 'multiple line filters'}   \n",
       "1                                                                {'chain_of_thought': 'The user query submits using multiple label filters: `application='openstack'`, `log_file_type='nova-compute'`. There are multiple line filters used sequentially: `|= '3edec1e4-9678-4a3a-a21b-a145a4ee5e61'`, `|= 'Took'`, `|= 'seconds to build instance'`, `| regexp '\\[instance: (?P<instance_id>[^\\]]+)\\d+] Took (?P<build_time>\\d+.\\d+) seconds to build instance'`. By definition, using several different types of line filters suggests it falls under 'multiple line filters'. For labels, using multiple labels as part of the stream selector puts this into the 'multiple log stream selectors' category.', 'label_filter': 'multiple log stream selectors', 'line_filter': 'multiple line filters'}   \n",
       "2                                              {'chain_of_thought': 'For the log query in question, there are two label filters specified: `application=\"openstack\"` and `log_file_type=\"nova-compute\"`. This clearly denotes the use of multiple label filters. Concerning line filters, three distinct line filtering expressions are present: `|= \"3416d0fa-6f0f-41ce-9c0a-59ae9a735da8\"`, `|~ \"Total (memory|disk|vcpu):\"`, and `| regexp \"\\[instance: (?P<instance_id>[^\\]]+)\\] Total (?P<resource>\\w+): (?P<total>\\d+(?:\\.\\d+)?) (?P<unit>\\w+), used: (?P<used>\\d+(?:\\.\\d+)?) \\w+\"`. Each of these targets a different element of the log entries, qualifying the query as having multiple line filters.', 'label_filter': 'multiple log stream selectors', 'line_filter': 'multiple line filters'}   \n",
       "3                                                                                                                                                                                                                          {'chain_of_thought': 'The query contains three label filters specified within curly braces: `application=\"openstack\"`, `log_file_type=\"nova-compute\"`, and `component=\"nova.compute.resource_tracker\"`. Moreover, there are two line filters used in sequence: `|=` to match the host `cp-1.slowvm1.tcloud-pg0.utah.cloudlab.us` and another `|=` to match the phrase `Final resource view:`. This classifies the query as having multiple label filters and multiple line filters.', 'label_filter': 'multiple log stream selectors', 'line_filter': 'multiple line filters'}   \n",
       "4                                                                                                                                                                                                                               {'chain_of_thought': 'This query has three label filters: `application=\"openstack\"`, `log_file_type=\"nova-compute\"`, `component=\"nova.compute.resource_tracker\"`. Additionally, there are two line filters `|=\"cp-1.slowvm1.tcloud-pg0.utah.cloudlab.us\"` and `|=\"Final resource view:\"` to target specific log entries, followed by a regex match filter `| regexp ...`. Given there are multiple label and line filters employed, both categories indicate multiple filters.', 'label_filter': 'multiple log stream selectors', 'line_filter': 'multiple line filters'}   \n",
       "..                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    ...   \n",
       "95  {'chain_of_thought': 'This query uses a single label filter: `component=\"dfs.FSNamesystem\"`, to select logs from the specified component. It also includes multiple line filters: the first line filter uses a regular expression to find log lines containing a specific pattern related to stored blocks `|~ \"BLOCK\\* NameSystem\\.addStoredBlock: blockMap updated:.*is added to.*size.*\"`. Following this, another regular expression is applied to extract the size from the log line `| regexp \"BLOCK\\* NameSystem\\.addStoredBlock: blockMap updated:.*is added to.*size (?P<size>[0-9]+)\"`, which further processes the log lines. Thus, the query has a single label filter and multiple line filters.', 'label_filter': 'single log stream selector', 'line_filter': 'multiple line filters'}   \n",
       "96                                                                                                                                                                                                                                      {'chain_of_thought': 'The query contains a single label filter: `component=\"dfs.FSNamesystem\"`. Additionally, it includes two line filters `|=\"BLOCK* Removing block\"` and `|=\"from neededReplications as it does not belong to any file\"` to specifically match log lines containing these strings. Since there is only one log stream selector and multiple line filters, this query falls into the categories of single log stream selector and multiple line filters.', 'label_filter': 'single log stream selector', 'line_filter': 'multiple line filters'}   \n",
       "97                                           {'chain_of_thought': 'The given LogQL query has a single label filter, which is {component=\"dfs.DataNode$BlockReceiver\"}. This selector identifies the specific component generating the logs. There are multiple line filters used in the query: one regular expression filter looking for a pattern with a 'SocketTimeoutException' and another regular expression extracting specific details about the block ID and timeout value using named groups. Additionally, there is a filter checking if the timeout exceeds a certain threshold. These elements indicate the use of multiple line filters since more than one type of line filtering is being applied.', 'label_filter': 'single log stream selector', 'line_filter': 'multiple line filters'}   \n",
       "98                                                                                                                                                                                                                                                                                                                                                                                                                                         {'chain_of_thought': 'This query includes a single label filter: `application=\"hdfs\"`. Additionally, it contains a single line filter `|~ \"PendingReplicationMonitor timed out block .*\"` used to match logs with a specific pattern. There are no multiple filters used.', 'label_filter': 'single log stream selector', 'line_filter': 'single line filter'}   \n",
       "99                                                                                                                                                                                                                                                                               {'chain_of_thought': 'The query has a single label filter: `application=\"hdfs\"`. The query is checking rates over log lines that match two different line filters: `|~ \"Transmitted block\"` and `|~ \"Starting thread to transfer block\"`. Since there are two different line filters used in separate sub-queries, this qualifies as multiple line filters. Therefore, the labels are single, and the line filters are multiple.', 'label_filter': 'single log stream selector', 'line_filter': 'multiple line filters'}   \n",
       "\n",
       "              line_filter                   label_filter  \\\n",
       "0   multiple line filters  multiple log stream selectors   \n",
       "1   multiple line filters  multiple log stream selectors   \n",
       "2   multiple line filters  multiple log stream selectors   \n",
       "3   multiple line filters  multiple log stream selectors   \n",
       "4   multiple line filters  multiple log stream selectors   \n",
       "..                    ...                            ...   \n",
       "95  multiple line filters     single log stream selector   \n",
       "96  multiple line filters     single log stream selector   \n",
       "97  multiple line filters     single log stream selector   \n",
       "98     single line filter     single log stream selector   \n",
       "99  multiple line filters     single log stream selector   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   metric_category_result  \\\n",
       "0                                                                                                                                                       {'categories': None, 'chain_of_thought': 'The provided query involves log matching and extraction using filters and regexp but does not perform any quantitative aggregations or statistical computations, such as sum, count, or rate on the extracted data. It includes log filters, a regexp pattern for extracting data, and formatting the output using line_format, which suggests manipulation of text rather than numerical computation for metrics. Therefore, there are no metric aggregations present in this query.'}   \n",
       "1                                                                                                                                                                                                                                              {'categories': None, 'chain_of_thought': 'This LogQL query does not contain any aggregation operators like `sum`, `avg`, `max`, `min`, `count`, etc. It appears to involve parsing and restructuring log lines with `regexp` and `line_format` but does not aggregate these logs into metrics. Therefore, it does not fall into the categories of metric aggregation, whether log range, unwrapped range, or built-in range aggregation.'}   \n",
       "2                                                                                                                                                                                                                                                    {'categories': None, 'chain_of_thought': 'The query provided does not include any metric aggregation functions like `sum()`, `count_over_time()`, `rate()`, etc., from LogQL's capabilities. It primarily filters and reformats log lines using operators like `|=`, `|~`, `| regexp`, and `| line_format`. There's no aggregation over time or conversion of log lines into numerical metrics for further statistical operations.'}   \n",
       "3                                                                 {'categories': ['unwrapped_range_aggregation', 'built_in_range_aggregation'], 'chain_of_thought': 'The query uses `max by (node)` which is a built-in aggregation operator that aggregates over the labels. The inner aggregation function `max_over_time` used after unwrapping `used_vcpus` indicates it is an unwrapped range aggregation because it operates on a labeled sample value over a specified time period. Both `max` aggregation operators are involved but in different contexts: one as part of the unwrapped range aggregation, and the other as a built-in range aggregation at the overall level.'}   \n",
       "4                                                                                                                                {'categories': ['built_in_range_aggregation', 'unwrapped_range_aggregation'], 'chain_of_thought': 'The query uses two `max` functions which are built-in aggregation operators. The outer `max by (node)` is a built-in aggregation to select maximum values grouped by `node` label. The inner `max_over_time` function operates on an unwrapped range aggregation, using the `unwrap` operator to extract `used_ram_mb` and consider it over a 30 day range. This clearly indicates a combination of both unwrapped and built-in range aggregations.'}   \n",
       "..                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    ...   \n",
       "95  {'categories': ['built_in_range_aggregation', 'unwrapped_range_aggregation'], 'chain_of_thought': 'In the user query, there is a combination of `sum()` and `sum_over_time()`. The `sum()` function is identified as a built-in aggregation operator, focusing on aggregating metrics based on conditions set within its parameters. The `sum_over_time()` function deals with unwrapped range aggregations where it aggregates values over a specified time period from an unwrapped label. In this case, the label `size` is unwrapped and then aggregated over `24h`. These two aggregation types distinguish the use of built-in and unwrapped range aggregations in the query.'}   \n",
       "96                                                                                                                                                                                                                      {'categories': ['log_range_aggregation'], 'chain_of_thought': 'In the query, the aggregation function used is `count_over_time` which is applied over a 12-hour range vector. This function is directly mentioned in the documentation under log range aggregations, where it's used to count the entries within a given range for each log stream. There are no unwrapped range aggregations or built-in aggregation operators directly applied in this query.'}   \n",
       "97                                                                                                                            {'categories': ['log_range_aggregation', 'built_in_range_aggregation'], 'chain_of_thought': 'In this query, two primary operations are observed. The `sum()` function is a built-in aggregation operator that aggregates values across a dataset. The other function, `count_over_time()`, which is used within the `sum()`, is a log range aggregation as it applies a time-based count over log entries. The `count_over_time()` is applied over a 24-hour window, which involves counting entries that match the log pattern and conditions specified.'}   \n",
       "98                                                                                                                                                                                                            {'categories': ['log_range_aggregation', 'built_in_range_aggregation'], 'chain_of_thought': 'The query uses the `sum()` function as well as the `count_over_time()` function over a logging range of 12 hours specified. According to the documentation, `count_over_time` is categorized as a log range aggregation as it counts log entries over a specified time range. `sum()` is a built-in aggregation operator, used here to aggregate the counts over all labels.'}   \n",
       "99                                                                                                                     {'categories': ['log_range_aggregation', 'built_in_range_aggregation'], 'chain_of_thought': 'In this query, `sum()` and `rate()` are used. From the LogQL documentation, `rate()` functions as a log range aggregation, calculating the rate of logs over a specified time period. The `sum()` function is a built-in aggregation operator used here to sum up the rates calculated. The entire expression calculates a rate over an hour and uses built-in aggregation operators to sum these rates. These sums are then combined in a mathematical expression.'}   \n",
       "\n",
       "                                              metric_category  \\\n",
       "0                                                        None   \n",
       "1                                                        None   \n",
       "2                                                        None   \n",
       "3   [unwrapped_range_aggregation, built_in_range_aggregation]   \n",
       "4   [built_in_range_aggregation, unwrapped_range_aggregation]   \n",
       "..                                                        ...   \n",
       "95  [built_in_range_aggregation, unwrapped_range_aggregation]   \n",
       "96                                    [log_range_aggregation]   \n",
       "97        [log_range_aggregation, built_in_range_aggregation]   \n",
       "98        [log_range_aggregation, built_in_range_aggregation]   \n",
       "99        [log_range_aggregation, built_in_range_aggregation]   \n",
       "\n",
       "                           variables application_variables  \\\n",
       "0         [instance_id, time_in_sec]         [application]   \n",
       "1         [instance_id, time_in_sec]         [application]   \n",
       "2            [instance_id, resource]         [application]   \n",
       "3       [compute_node, time_in_days]         [application]   \n",
       "4       [compute_node, time_in_days]         [application]   \n",
       "..                               ...                   ...   \n",
       "95                   [time_in_hours]         [application]   \n",
       "96                   [time_in_hours]         [application]   \n",
       "97  [time_in_minutes, time_in_hours]         [application]   \n",
       "98                   [time_in_hours]         [application]   \n",
       "99                   [time_in_hours]         [application]   \n",
       "\n",
       "                                                                  row_variables  \n",
       "0                                                     [instance_id, spawn_time]  \n",
       "1                                                     [instance_id, build_time]  \n",
       "2                                    [instance_id, resource, total, unit, used]  \n",
       "3   [node, total_ram, used_ram, total_disk, used_disk, total_vcpus, used_vcpus]  \n",
       "4   [node, total_ram, used_ram, total_disk, used_disk, total_vcpus, used_vcpus]  \n",
       "..                                                                          ...  \n",
       "95                                                              [time_in_hours]  \n",
       "96                                                              [time_in_hours]  \n",
       "97                                                     [timeout, time_in_hours]  \n",
       "98                                                              [time_in_hours]  \n",
       "99                                                              [time_in_hours]  \n",
       "\n",
       "[100 rows x 15 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(final_dataset.to_pandas())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
