{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Application Level Variations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "import instructor\n",
    "from openai import OpenAI\n",
    "\n",
    "client = instructor.from_openai(OpenAI())\n",
    "\n",
    "dataset_path = \"nl-logql-dataset-00\"\n",
    "dataset = Dataset.load_from_disk(dataset_path)\n",
    "\n",
    "df = dataset.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_colwidth\", 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{\"application\":\"openssh\",\"id\":59,\"question\":\"How many attempts were made to authenticate with invalid users from each unique source IP in the past 24 hours? (more than 200 attempts)\",\"logql_query\":\"sum(\\nsum by (source_ip) (\\n  count_over_time(\\n    {application=\\\"openssh\\\", hostname=\\\"LabSZ\\\"}\\n    |~ \\\"Failed password for invalid user\\\"\\n    | regexp \\\"Failed password for invalid user (?P<invalid_user>\\\\\\\\S+) from (?P<source_ip>\\\\\\\\d+\\\\\\\\.\\\\\\\\d+\\\\\\\\.\\\\\\\\d+\\\\\\\\.\\\\\\\\d+)\\\"\\n    | __error__=\\\"\\\"\\n    [24h]\\n  )\\n) > 200\\n)\",\"query_explanation\":\"1\\n{application=\\\"openssh\\\", hostname=\\\"LabSZ\\\"}\\nFetch all log lines matching label filters.\\n2\\n<expr> |~ `Failed password for invalid user`\\nReturn log lines that match a RE2 regex pattern. Failed password for invalid user.\\n\\n3\\n<expr> | regexp `Failed password for invalid user (?P<invalid_user>\\\\S+) from (?P<source_ip>\\\\d+\\\\.\\\\d+\\\\.\\\\d+\\\\.\\\\d+)`\\nThe regexp parser takes a single parameter | regexp \\\"<re>\\\" which is the regular expression using the Golang RE2 syntax. The regular expression must contain a least one named sub-match (e.g (?P<name>re)), each sub-match will extract a different label. The expression matches the structure of a log line. The extracted labels can be used in label filter expressions and used as values for a range aggregation via the unwrap operation.\\n\\n4\\n<expr> | __error__=``\\nFilter out all formatting and parsing errors.\\n\\n5\\ncount_over_time(<expr> [24h])\\nThe count of all values in the specified interval. The range vector is set to 24h.\\n\\n6\\nsum by(source_ip) (<expr>)\\nCalculates sum over dimensions while preserving label source_ip.\\n\\n7\\n<expr> > 200\",\"query_result\":\"11.5k\",\"category\":\"Invalid User Attempts\",\"log_category_result\":{\"chain_of_thought\":\"In this query, there are two label filters set up: `application=\\\"openssh\\\"` and `hostname=\\\"LabSZ\\\"`. This clearly indicates the use of multiple label filters. Additionally, the query contains two line filters: a simple substring filter `|~ \\\"Failed password for invalid user\\\"` and a regular expression filter `| regexp \\\"Failed password for invalid user (?P<invalid_user>\\\\S+) from (?P<source_ip>\\\\d+\\\\.\\\\d+\\\\.\\\\d+\\\\.\\\\d+)\\\"`. This combination places the query in the category of multiple line filters.\",\"label_filter\":\"multiple log stream selectors\",\"line_filter\":\"multiple line filters\"},\"line_filter\":\"multiple line filters\",\"label_filter\":\"multiple log stream selectors\",\"metric_category_result\":{\"categories\":[\"built_in_range_aggregation\",\"log_range_aggregation\"],\"chain_of_thought\":\"The query employs both `sum()` and `sum by (source_ip)`, which are built-in aggregation operators used in LogQL. Additionally, `count_over_time` specifies a log range aggregation as it uses a duration to aggregate log data over the specified time. Both types of metric aggregations are clearly evident in the query.\"},\"metric_category\":[\"built_in_range_aggregation\",\"log_range_aggregation\"],\"variables\":[\"num\",\"time_in_hours\"],\"application_variables\":[\"application\",\"hostname\"],\"row_variables\":[\"num\",\"time_in_hours\"]}]\n"
     ]
    }
   ],
   "source": [
    "print(df.sample(n=1).to_json(orient=\"records\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_columns = df.columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns_to_vary = [\"question\", \"logql_query\", \"query_explanation\", \"query_result\"]\n",
    "\n",
    "columns_to_vary = [\"question\", \"logql_query\", \"query_result\"]\n",
    "columns_to_keep = [col for col in all_columns if col not in columns_to_vary]\n",
    "\n",
    "columns_for_llm = columns_to_vary\n",
    "columns_for_llm.append(\"application_variables\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['question', 'logql_query', 'query_result', 'application_variables']\n",
      "['application', 'id', 'query_explanation', 'category', 'log_category_result', 'line_filter', 'label_filter', 'metric_category_result', 'metric_category', 'variables', 'application_variables', 'row_variables']\n",
      "['question', 'logql_query', 'query_result', 'application_variables']\n"
     ]
    }
   ],
   "source": [
    "print(columns_to_vary)\n",
    "print(columns_to_keep)\n",
    "print(columns_for_llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{\"question\":\"How many times did we encounter 'Corrupted MAC on input' errors in the last week, grouped by host?\",\"logql_query\":\"sum by (hostname) (count_over_time({application=\\\"openssh\\\"} |= \\\"Corrupted MAC on input\\\" [1w]))\",\"query_explanation\":\"1\\n{application=\\\"openssh\\\"}\\nFetch all log lines matching label filters.\\n2\\n<expr> |= `Corrupted MAC on input`\\nReturn log lines that contain string Corrupted MAC on input.\\n\\n3\\ncount_over_time(<expr> [1w])\\nThe count of all values in the specified interval. The range vector is set to 1w.\\n\\n4\\nsum by(hostname) (<expr>)\\nCalculates sum over dimensions while preserving label hostname.\",\"query_result\":\"1\",\"application_variables\":[\"application\",\"hostname\"]}]\n"
     ]
    }
   ],
   "source": [
    "print(df[columns_for_llm].sample(n=1).to_json(orient=\"records\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"You are a senior data analyst and data engineer tasked with generating variations of a dataset. Your goal is to read each row of the dataset, focus on specified columns, and create multiple variations while keeping certain columns constant.\n",
    "\n",
    "Here is the format you'll receive the dataset row in:\n",
    "<dataset>\n",
    "{{DATASET}}\n",
    "</dataset>\n",
    "\n",
    "Here is the format you'll receive the columns you should vary be\n",
    "<columns_to_vary>\n",
    "{{COLUMNS_TO_VARY}}\n",
    "</columns_to_vary>\n",
    "\n",
    "Here is the format you'll receive the columns that should remain constant in:\n",
    "<columns_to_keep>\n",
    "{{COLUMNS_TO_KEEP}}\n",
    "</columns_to_keep>\n",
    "\n",
    "You are to generate {{NUM_VARIATIONS}} variations for each row in the dataset.\n",
    "\n",
    "Important notes:\n",
    "1. Pay special attention to the 'application_variables' column, as it determines which variables should be changed for each row.\n",
    "2. The dataset contains LogQL queries. You should be able to understand and infer LogQL (Grafana Loki's Log Query Language) when generating variations.\n",
    "3. `query_explanation` column breaks down and explains the LogQL queries.\n",
    "\n",
    "Follow these steps to generate variations:\n",
    "1. Read each row of the dataset.\n",
    "2. Identify the 'application' for the current row.\n",
    "3. Identify and determine the `application_variables`  which variables need to be changed.\n",
    "4. Generate {{NUM_VARIATIONS}} new values for each variable that needs to be changed.\n",
    "5. Create {{NUM_VARIATIONS}} new rows based on the original row, replacing the variable values with the newly generated ones.\n",
    "6. Ensure that the columns specified in <columns_to_keep> remain unchanged in all variations.\n",
    "7. Make sure to adjust the query accordingly as it references the changed variables.\n",
    "\n",
    "Follow these notes on \"what\" the variations should be:\n",
    "1. \"application\" variables should be renamed as if they are different tenants and instances of that application. For eg:\n",
    "'openssh' -> \"openssh-us-east', 'openssh-tenant-1', ...\n",
    "2. Rewrite the `question` column to always explicitly mention the \"application\" variable.\n",
    "3. Rewrite all \"columns_to_vary\"  wherever possible to explicitly mention the \"application\" variable.\n",
    "\n",
    "When handling LogQL queries:\n",
    "1. Analyze the query to identify any references to variables that are being changed.\n",
    "2. Modify the query to use the new variable values in each variation.\n",
    "3. Ensure that the modified query is still valid LogQL syntax.\n",
    "4. Update the 'query_explanation' to reflect any changes made to the query.\n",
    "\n",
    "For each variation you generate, provide the following output:\n",
    "\n",
    "<variation>\n",
    "<original_row>\n",
    "Include the original row data here\n",
    "</original_row>\n",
    "<varied_row>\n",
    "Include the varied row data here, with changed variables and updated LogQL query if applicable\n",
    "</varied_row>\n",
    "<changes_made>\n",
    "Explain which variables were changed and how they affect the row data and LogQL query\n",
    "</changes_made>\n",
    "</variation>\n",
    "\n",
    "Repeat this process for each row in the dataset, generating {{NUM_VARIATIONS}} variations for each row. Ensure that your variations maintain the integrity and logic of the original data while introducing meaningful changes to the specified variables.\n",
    "\n",
    "Here is an example for you:\n",
    "## example\n",
    "<dataset>\n",
    "[{\"question\":\"How many unique users experienced authentication failures from the IP address 5.36.59.76?\",\"logql_query\":\"count(\\nsum(count_over_time({application=\\\"openssh\\\", hostname=\\\"LabSZ\\\"}\\n|= ip(\\\"5.36.59.76\\\")\\n| regexp \\\"(?P<message>(Failed password for (invalid user )?(?P<user>\\\\\\\\S+)|message repeated (?P<repeat_count>\\\\\\\\d+) times: \\\\\\\\[ Failed password for (invalid user )?(?P<repeated_user>\\\\\\\\S+))) from 5\\\\\\\\.36\\\\\\\\.59\\\\\\\\.76 port (?P<port>\\\\\\\\d+) ssh2\\\"\\n| __error__=\\\"\\\"\\n| label_format user=\\\"{{ or .user .repeated_user }}\\\"\\n| __error__=\\\"\\\"\\n[30d])) by (user))\",\"query_explanation\":\"1\\n{application=\\\"openssh\\\", hostname=\\\"LabSZ\\\"}\\nFetch all log lines matching label filters.\\n2\\n<expr> |= ip(`5.36.59.76`)\\nReturn log lines using IP matching of 5.36.59.76\\n\\n3\\n<expr> | regexp `(?P<message>(Failed password for (invalid user )?(?P<user>\\\\S+)|message repeated (?P<repeat_count>\\\\d+) times: \\\\[ Failed password for (invalid user )?(?P<repeated_user>\\\\S+))) from 5\\\\.36\\\\.59\\\\.76 port (?P<port>\\\\d+) ssh2`\\nThe regexp parser takes a single parameter | regexp \\\"<re>\\\" which is the regular expression using the Golang RE2 syntax. The regular expression must contain a least one named sub-match (e.g (?P<name>re)), each sub-match will extract a different label. The expression matches the structure of a log line. The extracted labels can be used in label filter expressions and used as values for a range aggregation via the unwrap operation.\\n\\n4\\n<expr> | __error__=``\\nFilter out all formatting and parsing errors.\\n\\n5\\n<expr> | label_format user=\\\"{{ or .user .repeated_user }}\\\"\\nThis will change name of label to desired new label. In the example below, label \\\"error_level\\\" will be renamed to \\\"level\\\".\\n\\nExample: error_level=`level`\\n\\nRead the docs for more.\\n\\n6\\n<expr> | __error__=``\\nFilter out all formatting and parsing errors.\\n\\n7\\ncount_over_time(<expr> [30d])\\nThe count of all values in the specified interval. The range vector is set to 30d.\\n\\n8\\nsum by(user) (<expr>)\\nCalculates sum over dimensions while preserving label user.\\n\\n9\\ncount(<expr>)\\nCalculates count over the dimensions.\",\"query_result\":\"1\",\"application_variables\":[\"application\",\"hostname\"]}]\n",
    "</dataset>\n",
    "\n",
    "<columns_to_vary>\n",
    "['question', 'logql_query', 'query_explanation', 'query_result']\n",
    "</columns_to_vary>\n",
    "\n",
    "<columns_to_keep>\n",
    "['application', 'id', 'category', 'log_category_result', 'line_filter', 'label_filter', 'metric_category_result', 'metric_category', 'variables', 'application_variables', 'row_variables']\n",
    "</columns_to_keep>\n",
    "\n",
    "<variables_column>\n",
    "application_variables\n",
    "</variables_column>\n",
    "\n",
    "You are to generate 3 variations for each row in the dataset.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "USER_PROMPT = \"\"\"\n",
    "<dataset>\n",
    "{DATASET}\n",
    "</dataset>\n",
    "\n",
    "<columns_to_vary>\n",
    "{COLUMNS_TO_VARY}\n",
    "</columns_to_vary>\n",
    "\n",
    "<columns_to_keep>\n",
    "{COLUMNS_TO_KEEP}\n",
    "</columns_to_keep>\n",
    "\n",
    "<variables_column>\n",
    "{VARIABLES}\n",
    "</variables_column>\n",
    "\n",
    "You are to generate 5 variations for each row in the dataset.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<dataset>\n",
      "l\n",
      "</dataset>\n",
      "\n",
      "<columns_to_vary>\n",
      "['question', 'logql_query', 'query_result', 'application_variables']\n",
      "</columns_to_vary>\n",
      "\n",
      "<columns_to_keep>\n",
      "['application', 'id', 'query_explanation', 'category', 'log_category_result', 'line_filter', 'label_filter', 'metric_category_result', 'metric_category', 'variables', 'application_variables', 'row_variables']\n",
      "</columns_to_keep>\n",
      "\n",
      "<variables_column>\n",
      "['row_variables']\n",
      "</variables_column>\n",
      "\n",
      "You are to generate 5 variations for each row in the dataset.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    USER_PROMPT.format(\n",
    "        DATASET=\"l\",\n",
    "        COLUMNS_TO_VARY=columns_to_vary,\n",
    "        COLUMNS_TO_KEEP=columns_to_keep,\n",
    "        VARIABLES=[\"row_variables\"],\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field, field_validator\n",
    "from typing import List, Dict\n",
    "from instructor.utils import disable_pydantic_error_url\n",
    "\n",
    "disable_pydantic_error_url()\n",
    "\n",
    "\n",
    "class Row(BaseModel):\n",
    "    chain_of_thought: str = Field(\n",
    "        description=\"Your step-by-step thought process of each Variation, the variables changed and the fields where they were changed\"\n",
    "    )\n",
    "    question: str\n",
    "    logql_query: str\n",
    "    query_explanation: str\n",
    "    query_result: str\n",
    "\n",
    "\n",
    "def generate_synthetic_data(row: Dict):\n",
    "    # synthetic_rows = [row]\n",
    "    synthetic_rows = []\n",
    "\n",
    "    try:\n",
    "        rows = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            response_model=List[Row],\n",
    "            # max_retries=2,\n",
    "            temperature=0.1,\n",
    "            max_tokens=8192,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": USER_PROMPT.format(\n",
    "                        DATASET=row,\n",
    "                        COLUMNS_TO_VARY=columns_to_vary,\n",
    "                        COLUMNS_TO_KEEP=columns_to_keep,\n",
    "                        VARIABLES=row[\"row_variables\"],\n",
    "                    ),\n",
    "                },\n",
    "            ],\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        synthetic_row = row.copy()\n",
    "        del synthetic_row[\"id\"]\n",
    "        return {\"synthetic_rows\": synthetic_rows}\n",
    "\n",
    "    for _row in rows:\n",
    "        synthetic_row = row.copy()\n",
    "        del synthetic_row[\"id\"]\n",
    "        synthetic_row[\"question\"] = _row.question\n",
    "        synthetic_row[\"logql_query\"] = _row.logql_query\n",
    "        synthetic_row[\"query_explanation\"] = _row.query_explanation\n",
    "        synthetic_row[\"query_result\"] = _row.query_result\n",
    "\n",
    "        synthetic_rows.append(synthetic_row)\n",
    "    return {\"synthetic_rows\": synthetic_rows}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sidbin/Main-Quests/sauron/dataset-curation/.venv/lib/python3.12/site-packages/dill/_dill.py:414: PicklingWarning: Cannot locate reference to <class '__main__.Row'>.\n",
      "  StockPickler.save(self, obj, save_persistent_id)\n",
      "/home/sidbin/Main-Quests/sauron/dataset-curation/.venv/lib/python3.12/site-packages/dill/_dill.py:414: PicklingWarning: Cannot pickle <class '__main__.Row'>: __main__.Row has recursive self-references that trigger a RecursionError.\n",
      "  StockPickler.save(self, obj, save_persistent_id)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c2257e074bc4adc916a8d9de59d1964",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=20):   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RetryError[<Future at 0x767a12baee10 state=finished raised ValidationError>]\n",
      "RetryError[<Future at 0x767997519a90 state=finished raised ValidationError>]\n",
      "RetryError[<Future at 0x767a12d93650 state=finished raised ValidationError>]\n",
      "RetryError[<Future at 0x767994d5b050 state=finished raised ValidationError>]\n",
      "RetryError[<Future at 0x76799747f620 state=finished raised IncompleteOutputException>]\n",
      "RetryError[<Future at 0x767997478b90 state=finished raised IncompleteOutputException>]\n",
      "RetryError[<Future at 0x767994d971d0 state=finished raised IncompleteOutputException>]\n",
      "RetryError[<Future at 0x767a12bb35c0 state=finished raised ValidationError>]\n",
      "RetryError[<Future at 0x76799746d0a0 state=finished raised IncompleteOutputException>]\n",
      "RetryError[<Future at 0x767994d3d0d0 state=finished raised ValidationError>]\n",
      "RetryError[<Future at 0x767a12bb1940 state=finished raised ValidationError>]\n",
      "RetryError[<Future at 0x76799747f620 state=finished raised IncompleteOutputException>]\n",
      "RetryError[<Future at 0x767a12baaae0 state=finished raised ValidationError>]\n",
      "RetryError[<Future at 0x76799747a0c0 state=finished raised ValidationError>]\n",
      "RetryError[<Future at 0x767997455310 state=finished raised ValidationError>]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "synthetic_dataset = dataset.map(generate_synthetic_data, num_proc=os.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d69070b91b14f1ea552d017987a81a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "synthetic_dataset = synthetic_dataset.map(remove_columns=dataset.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'application': 'openstack',\n",
       " 'application_variables': ['application'],\n",
       " 'category': '',\n",
       " 'label_filter': 'multiple log stream selectors',\n",
       " 'line_filter': 'multiple line filters',\n",
       " 'log_category_result': {'chain_of_thought': 'Analyzing the query, it contains three label filters: `application=\"openstack\"`, `log_file_type=\"nova-compute\"`, `component=\"nova.compute.manager\"`. Additionally, there are three line filters (`|= \"3edec1e4-9678-4a3a-a21b-a145a4ee5e61\"`, `|= \"Took\"`, `|= \"seconds to spawn the instance on the hypervisor\"`) followed by a regex operation and a line format operation. The presence of three label filters classifies this under multiple log stream selectors, and the presence of more than one line filter classifies it under multiple line filters.',\n",
       "  'label_filter': 'multiple log stream selectors',\n",
       "  'line_filter': 'multiple line filters'},\n",
       " 'logql_query': '{application=\"openstack-us-east\", log_file_type=\"nova-compute\", component=\"nova.compute.manager\"} |= \"3edec1e4-9678-4a3a-a21b-a145a4ee5e61\" |= \"Took\" |= \"seconds to spawn the instance on the hypervisor\" | regexp \"\\\\\\\\[instance: (?P<instance_id>[^\\\\\\\\]]+)\\\\\\\\] Took (?P<spawn_time>\\\\\\\\d+\\\\\\\\.\\\\\\\\d+) seconds to spawn the instance on the hypervisor\" | line_format \"{{.instance_id}} took {{.spawn_time}}\"',\n",
       " 'metric_category': None,\n",
       " 'metric_category_result': {'categories': None,\n",
       "  'chain_of_thought': 'The provided query involves log matching and extraction using filters and regexp but does not perform any quantitative aggregations or statistical computations, such as sum, count, or rate on the extracted data. It includes log filters, a regexp pattern for extracting data, and formatting the output using line_format, which suggests manipulation of text rather than numerical computation for metrics. Therefore, there are no metric aggregations present in this query.'},\n",
       " 'query_explanation': 'bla',\n",
       " 'query_result': '3edec1e4-9678-4a3a-a21b-a145a4ee5e61 took 20.58',\n",
       " 'question': 'How long did it take to spawn instance 3edec1e4-9678-4a3a-a21b-a145a4ee5e61 on the hypervisor for openstack-us-east?',\n",
       " 'row_variables': ['instance_id', 'spawn_time'],\n",
       " 'variables': ['instance_id', 'time_in_sec']}"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synthetic_dataset[\"synthetic_rows\"][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "synthetic_df = synthetic_dataset.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([{'application': 'openstack', 'application_variables': array(['application'], dtype=object), 'category': '', 'label_filter': 'multiple log stream selectors', 'line_filter': 'multiple line filters', 'log_category_result': {'chain_of_thought': 'Analyzing the query, it contains three label filters: `application=\"openstack\"`, `log_file_type=\"nova-compute\"`, `component=\"nova.compute.manager\"`. Additionally, there are three line filters (`|= \"3edec1e4-9678-4a3a-a21b-a145a4ee5e61\"`, `|= \"Took\"`, `|= \"seconds to spawn the instance on the hypervisor\"`) followed by a regex operation and a line format operation. The presence of three label filters classifies this under multiple log stream selectors, and the presence of more than one line filter classifies it under multiple line filters.', 'label_filter': 'multiple log stream selectors', 'line_filter': 'multiple line filters'}, 'logql_query': '{application=\"openstack-us-east\", log_file_type=\"nova-compute\", component=\"nova.compute.manager\"} |= \"3edec1e4-9678-4a3a-a21b-a145a4ee5e61\" |= \"Took\" |= \"seconds to spawn the instance on the hypervisor\" | regexp \"\\\\\\\\[instance: (?P<instance_id>[^\\\\\\\\]]+)\\\\\\\\] Took (?P<spawn_time>\\\\\\\\d+\\\\\\\\.\\\\\\\\d+) seconds to spawn the instance on the hypervisor\" | line_format \"{{.instance_id}} took {{.spawn_time}}\"', 'metric_category': None, 'metric_category_result': {'categories': None, 'chain_of_thought': 'The provided query involves log matching and extraction using filters and regexp but does not perform any quantitative aggregations or statistical computations, such as sum, count, or rate on the extracted data. It includes log filters, a regexp pattern for extracting data, and formatting the output using line_format, which suggests manipulation of text rather than numerical computation for metrics. Therefore, there are no metric aggregations present in this query.'}, 'query_explanation': 'bla', 'query_result': '3edec1e4-9678-4a3a-a21b-a145a4ee5e61 took 20.58', 'question': 'How long did it take to spawn instance 3edec1e4-9678-4a3a-a21b-a145a4ee5e61 on the hypervisor for openstack-us-east?', 'row_variables': array(['instance_id', 'spawn_time'], dtype=object), 'variables': array(['instance_id', 'time_in_sec'], dtype=object)},\n",
       "       {'application': 'openstack', 'application_variables': array(['application'], dtype=object), 'category': '', 'label_filter': 'multiple log stream selectors', 'line_filter': 'multiple line filters', 'log_category_result': {'chain_of_thought': 'Analyzing the query, it contains three label filters: `application=\"openstack\"`, `log_file_type=\"nova-compute\"`, `component=\"nova.compute.manager\"`. Additionally, there are three line filters (`|= \"3edec1e4-9678-4a3a-a21b-a145a4ee5e61\"`, `|= \"Took\"`, `|= \"seconds to spawn the instance on the hypervisor\"`) followed by a regex operation and a line format operation. The presence of three label filters classifies this under multiple log stream selectors, and the presence of more than one line filter classifies it under multiple line filters.', 'label_filter': 'multiple log stream selectors', 'line_filter': 'multiple line filters'}, 'logql_query': '{application=\"openstack-tenant-1\", log_file_type=\"nova-compute\", component=\"nova.compute.manager\"} |= \"3edec1e4-9678-4a3a-a21b-a145a4ee5e61\" |= \"Took\" |= \"seconds to spawn the instance on the hypervisor\" | regexp \"\\\\\\\\[instance: (?P<instance_id>[^\\\\\\\\]]+)\\\\\\\\] Took (?P<spawn_time>\\\\\\\\d+\\\\\\\\.\\\\\\\\d+) seconds to spawn the instance on the hypervisor\" | line_format \"{{.instance_id}} took {{.spawn_time}}\"', 'metric_category': None, 'metric_category_result': {'categories': None, 'chain_of_thought': 'The provided query involves log matching and extraction using filters and regexp but does not perform any quantitative aggregations or statistical computations, such as sum, count, or rate on the extracted data. It includes log filters, a regexp pattern for extracting data, and formatting the output using line_format, which suggests manipulation of text rather than numerical computation for metrics. Therefore, there are no metric aggregations present in this query.'}, 'query_explanation': 'bla', 'query_result': '3edec1e4-9678-4a3a-a21b-a145a4ee5e61 took 20.58', 'question': 'How long did it take to spawn instance 3edec1e4-9678-4a3a-a21b-a145a4ee5e61 on the hypervisor for openstack-tenant-1?', 'row_variables': array(['instance_id', 'spawn_time'], dtype=object), 'variables': array(['instance_id', 'time_in_sec'], dtype=object)},\n",
       "       {'application': 'openstack', 'application_variables': array(['application'], dtype=object), 'category': '', 'label_filter': 'multiple log stream selectors', 'line_filter': 'multiple line filters', 'log_category_result': {'chain_of_thought': 'Analyzing the query, it contains three label filters: `application=\"openstack\"`, `log_file_type=\"nova-compute\"`, `component=\"nova.compute.manager\"`. Additionally, there are three line filters (`|= \"3edec1e4-9678-4a3a-a21b-a145a4ee5e61\"`, `|= \"Took\"`, `|= \"seconds to spawn the instance on the hypervisor\"`) followed by a regex operation and a line format operation. The presence of three label filters classifies this under multiple log stream selectors, and the presence of more than one line filter classifies it under multiple line filters.', 'label_filter': 'multiple log stream selectors', 'line_filter': 'multiple line filters'}, 'logql_query': '{application=\"openstack-eu-west\", log_file_type=\"nova-compute\", component=\"nova.compute.manager\"} |= \"3edec1e4-9678-4a3a-a21b-a145a4ee5e61\" |= \"Took\" |= \"seconds to spawn the instance on the hypervisor\" | regexp \"\\\\\\\\[instance: (?P<instance_id>[^\\\\\\\\]]+)\\\\\\\\] Took (?P<spawn_time>\\\\\\\\d+\\\\\\\\.\\\\\\\\d+) seconds to spawn the instance on the hypervisor\" | line_format \"{{.instance_id}} took {{.spawn_time}}\"', 'metric_category': None, 'metric_category_result': {'categories': None, 'chain_of_thought': 'The provided query involves log matching and extraction using filters and regexp but does not perform any quantitative aggregations or statistical computations, such as sum, count, or rate on the extracted data. It includes log filters, a regexp pattern for extracting data, and formatting the output using line_format, which suggests manipulation of text rather than numerical computation for metrics. Therefore, there are no metric aggregations present in this query.'}, 'query_explanation': 'bla', 'query_result': '3edec1e4-9678-4a3a-a21b-a145a4ee5e61 took 20.58', 'question': 'How long did it take to spawn instance 3edec1e4-9678-4a3a-a21b-a145a4ee5e61 on the hypervisor for openstack-eu-west?', 'row_variables': array(['instance_id', 'spawn_time'], dtype=object), 'variables': array(['instance_id', 'time_in_sec'], dtype=object)},\n",
       "       {'application': 'openstack', 'application_variables': array(['application'], dtype=object), 'category': '', 'label_filter': 'multiple log stream selectors', 'line_filter': 'multiple line filters', 'log_category_result': {'chain_of_thought': 'Analyzing the query, it contains three label filters: `application=\"openstack\"`, `log_file_type=\"nova-compute\"`, `component=\"nova.compute.manager\"`. Additionally, there are three line filters (`|= \"3edec1e4-9678-4a3a-a21b-a145a4ee5e61\"`, `|= \"Took\"`, `|= \"seconds to spawn the instance on the hypervisor\"`) followed by a regex operation and a line format operation. The presence of three label filters classifies this under multiple log stream selectors, and the presence of more than one line filter classifies it under multiple line filters.', 'label_filter': 'multiple log stream selectors', 'line_filter': 'multiple line filters'}, 'logql_query': '{application=\"openstack-asia-pacific\", log_file_type=\"nova-compute\", component=\"nova.compute.manager\"} |= \"3edec1e4-9678-4a3a-a21b-a145a4ee5e61\" |= \"Took\" |= \"seconds to spawn the instance on the hypervisor\" | regexp \"\\\\\\\\[instance: (?P<instance_id>[^\\\\\\\\]]+)\\\\\\\\] Took (?P<spawn_time>\\\\\\\\d+\\\\\\\\.\\\\\\\\d+) seconds to spawn the instance on the hypervisor\" | line_format \"{{.instance_id}} took {{.spawn_time}}\"', 'metric_category': None, 'metric_category_result': {'categories': None, 'chain_of_thought': 'The provided query involves log matching and extraction using filters and regexp but does not perform any quantitative aggregations or statistical computations, such as sum, count, or rate on the extracted data. It includes log filters, a regexp pattern for extracting data, and formatting the output using line_format, which suggests manipulation of text rather than numerical computation for metrics. Therefore, there are no metric aggregations present in this query.'}, 'query_explanation': 'bla', 'query_result': '3edec1e4-9678-4a3a-a21b-a145a4ee5e61 took 20.58', 'question': 'How long did it take to spawn instance 3edec1e4-9678-4a3a-a21b-a145a4ee5e61 on the hypervisor for openstack-asia-pacific?', 'row_variables': array(['instance_id', 'spawn_time'], dtype=object), 'variables': array(['instance_id', 'time_in_sec'], dtype=object)},\n",
       "       {'application': 'openstack', 'application_variables': array(['application'], dtype=object), 'category': '', 'label_filter': 'multiple log stream selectors', 'line_filter': 'multiple line filters', 'log_category_result': {'chain_of_thought': 'Analyzing the query, it contains three label filters: `application=\"openstack\"`, `log_file_type=\"nova-compute\"`, `component=\"nova.compute.manager\"`. Additionally, there are three line filters (`|= \"3edec1e4-9678-4a3a-a21b-a145a4ee5e61\"`, `|= \"Took\"`, `|= \"seconds to spawn the instance on the hypervisor\"`) followed by a regex operation and a line format operation. The presence of three label filters classifies this under multiple log stream selectors, and the presence of more than one line filter classifies it under multiple line filters.', 'label_filter': 'multiple log stream selectors', 'line_filter': 'multiple line filters'}, 'logql_query': '{application=\"openstack-south-america\", log_file_type=\"nova-compute\", component=\"nova.compute.manager\"} |= \"3edec1e4-9678-4a3a-a21b-a145a4ee5e61\" |= \"Took\" |= \"seconds to spawn the instance on the hypervisor\" | regexp \"\\\\\\\\[instance: (?P<instance_id>[^\\\\\\\\]]+)\\\\\\\\] Took (?P<spawn_time>\\\\\\\\d+\\\\\\\\.\\\\\\\\d+) seconds to spawn the instance on the hypervisor\" | line_format \"{{.instance_id}} took {{.spawn_time}}\"', 'metric_category': None, 'metric_category_result': {'categories': None, 'chain_of_thought': 'The provided query involves log matching and extraction using filters and regexp but does not perform any quantitative aggregations or statistical computations, such as sum, count, or rate on the extracted data. It includes log filters, a regexp pattern for extracting data, and formatting the output using line_format, which suggests manipulation of text rather than numerical computation for metrics. Therefore, there are no metric aggregations present in this query.'}, 'query_explanation': 'bla', 'query_result': '3edec1e4-9678-4a3a-a21b-a145a4ee5e61 took 20.58', 'question': 'How long did it take to spawn instance 3edec1e4-9678-4a3a-a21b-a145a4ee5e61 on the hypervisor for openstack-south-america?', 'row_variables': array(['instance_id', 'spawn_time'], dtype=object), 'variables': array(['instance_id', 'time_in_sec'], dtype=object)}],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synthetic_df[\"synthetic_rows\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "expanded_df = pd.DataFrame(\n",
    "    [item for row in synthetic_df[\"synthetic_rows\"] for item in row]\n",
    ")\n",
    "\n",
    "# Reset the index if needed\n",
    "expanded_df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "expanded_df.rename(columns={\"index\": \"id\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_synthetic_dataset = Dataset.from_pandas(expanded_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'application', 'application_variables', 'category', 'label_filter', 'line_filter', 'log_category_result', 'logql_query', 'metric_category', 'metric_category_result', 'query_explanation', 'query_result', 'question', 'row_variables', 'variables'],\n",
       "    num_rows: 424\n",
       "})"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_synthetic_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e0ee58a6ed948cb9b2f73bf9e44721a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/424 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "final_synthetic_dataset.save_to_disk(\"nl-logql-dataset-02\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24b924f79ec64d9caa96a9feea8a5a94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33b81242a8624b9094077e7509e38f18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e2208c91d764a5bbd663f794e2f5ab4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/1.09k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/sidbin/natural-logql/commit/d9ac961023083e4a526a30f7f412a595556f4b5d', commit_message='Upload dataset', commit_description='', oid='d9ac961023083e4a526a30f7f412a595556f4b5d', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/sidbin/natural-logql', endpoint='https://huggingface.co', repo_type='dataset', repo_id='sidbin/natural-logql'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_synthetic_dataset.push_to_hub(\"sidbin/natural-logql\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def generate_synthetic_data(row: Dict) -> List[Dict]:\n",
    "    synthetic_rows = []\n",
    "\n",
    "    try:\n",
    "        rows = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            response_model=List[Row],\n",
    "            temperature=0.1,\n",
    "            max_tokens=8192,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": USER_PROMPT.format(\n",
    "                        DATASET=row,\n",
    "                        COLUMNS_TO_VARY=columns_to_vary,\n",
    "                        COLUMNS_TO_KEEP=columns_to_keep,\n",
    "                        VARIABLES=row[\"row_variables\"],\n",
    "                    ),\n",
    "                },\n",
    "            ],\n",
    "        )\n",
    "\n",
    "        for _row in rows:\n",
    "            synthetic_row = row.copy()\n",
    "            synthetic_row[\"question\"] = _row.question\n",
    "            synthetic_row[\"logql_query\"] = _row.logql_query\n",
    "            synthetic_row[\"query_explanation\"] = _row.query_explanation\n",
    "            synthetic_row[\"query_result\"] = _row.query_result\n",
    "            synthetic_rows.append(synthetic_row)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating synthetic data: {e}\")\n",
    "\n",
    "    return synthetic_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "generating synthetic data:   0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "generating synthetic data: 100%|██████████| 100/100 [32:17<00:00, 19.37s/it]\n"
     ]
    }
   ],
   "source": [
    "# df = dataset.shuffle(42).select(range(2)).to_pandas()\n",
    "df = dataset.to_pandas()\n",
    "tqdm.pandas(desc=\"generating synthetic data\")\n",
    "expanded_rows = df.progress_apply(\n",
    "    lambda row: generate_synthetic_data(row.to_dict()), axis=1\n",
    ")\n",
    "\n",
    "expanded_df = pd.DataFrame([item for sublist in expanded_rows for item in sublist])\n",
    "\n",
    "expanded_dataset = Dataset.from_pandas(expanded_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4fbb8edea3545799be943ac85b56198",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/300 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "expanded_dataset.save_to_disk(\"nl-logql-dataset-01\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>application</th>\n",
       "      <th>id</th>\n",
       "      <th>question</th>\n",
       "      <th>logql_query</th>\n",
       "      <th>query_explanation</th>\n",
       "      <th>query_result</th>\n",
       "      <th>category</th>\n",
       "      <th>log_category_result</th>\n",
       "      <th>line_filter</th>\n",
       "      <th>label_filter</th>\n",
       "      <th>metric_category_result</th>\n",
       "      <th>metric_category</th>\n",
       "      <th>variables</th>\n",
       "      <th>application_variables</th>\n",
       "      <th>row_variables</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>openstack</td>\n",
       "      <td>2</td>\n",
       "      <td>How long did it take to spawn instance 3edec1e4-9678-4a3a-a21b-a145a4ee5e61 on the hypervisor for openstack-us-east?</td>\n",
       "      <td>{application=\"openstack-us-east\", log_file_type=\"nova-compute\", component=\"nova.compute.manager\"} |= \"3edec1e4-9678-4a3a-a21b-a145a4ee5e61\" |= \"Took\" |= \"seconds to spawn the instance on the hypervisor\" | regexp \"\\\\[instance: (?P&lt;instance_id&gt;[^\\\\]]+)\\\\] Took (?P&lt;spawn_time&gt;\\\\d+\\\\.\\\\d+) seconds to spawn the instance on the hypervisor\" | line_format \"{{.instance_id}} took {{.spawn_time}}\"</td>\n",
       "      <td>bla</td>\n",
       "      <td>3edec1e4-9678-4a3a-a21b-a145a4ee5e61 took 20.58</td>\n",
       "      <td></td>\n",
       "      <td>{'chain_of_thought': 'Analyzing the query, it contains three label filters: `application=\"openstack\"`, `log_file_type=\"nova-compute\"`, `component=\"nova.compute.manager\"`. Additionally, there are three line filters (`|= \"3edec1e4-9678-4a3a-a21b-a145a4ee5e61\"`, `|= \"Took\"`, `|= \"seconds to spawn the instance on the hypervisor\"`) followed by a regex operation and a line format operation. The presence of three label filters classifies this under multiple log stream selectors, and the presence of more than one line filter classifies it under multiple line filters.', 'label_filter': 'multiple log stream selectors', 'line_filter': 'multiple line filters'}</td>\n",
       "      <td>multiple line filters</td>\n",
       "      <td>multiple log stream selectors</td>\n",
       "      <td>{'categories': None, 'chain_of_thought': 'The provided query involves log matching and extraction using filters and regexp but does not perform any quantitative aggregations or statistical computations, such as sum, count, or rate on the extracted data. It includes log filters, a regexp pattern for extracting data, and formatting the output using line_format, which suggests manipulation of text rather than numerical computation for metrics. Therefore, there are no metric aggregations present in this query.'}</td>\n",
       "      <td>None</td>\n",
       "      <td>[instance_id, time_in_sec]</td>\n",
       "      <td>[application]</td>\n",
       "      <td>[instance_id, spawn_time]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>openstack</td>\n",
       "      <td>2</td>\n",
       "      <td>How long did it take to spawn instance 3edec1e4-9678-4a3a-a21b-a145a4ee5e61 on the hypervisor for openstack-tenant-1?</td>\n",
       "      <td>{application=\"openstack-tenant-1\", log_file_type=\"nova-compute\", component=\"nova.compute.manager\"} |= \"3edec1e4-9678-4a3a-a21b-a145a4ee5e61\" |= \"Took\" |= \"seconds to spawn the instance on the hypervisor\" | regexp \"\\\\[instance: (?P&lt;instance_id&gt;[^\\\\]]+)\\\\] Took (?P&lt;spawn_time&gt;\\\\d+\\\\.\\\\d+) seconds to spawn the instance on the hypervisor\" | line_format \"{{.instance_id}} took {{.spawn_time}}\"</td>\n",
       "      <td>bla</td>\n",
       "      <td>3edec1e4-9678-4a3a-a21b-a145a4ee5e61 took 20.58</td>\n",
       "      <td></td>\n",
       "      <td>{'chain_of_thought': 'Analyzing the query, it contains three label filters: `application=\"openstack\"`, `log_file_type=\"nova-compute\"`, `component=\"nova.compute.manager\"`. Additionally, there are three line filters (`|= \"3edec1e4-9678-4a3a-a21b-a145a4ee5e61\"`, `|= \"Took\"`, `|= \"seconds to spawn the instance on the hypervisor\"`) followed by a regex operation and a line format operation. The presence of three label filters classifies this under multiple log stream selectors, and the presence of more than one line filter classifies it under multiple line filters.', 'label_filter': 'multiple log stream selectors', 'line_filter': 'multiple line filters'}</td>\n",
       "      <td>multiple line filters</td>\n",
       "      <td>multiple log stream selectors</td>\n",
       "      <td>{'categories': None, 'chain_of_thought': 'The provided query involves log matching and extraction using filters and regexp but does not perform any quantitative aggregations or statistical computations, such as sum, count, or rate on the extracted data. It includes log filters, a regexp pattern for extracting data, and formatting the output using line_format, which suggests manipulation of text rather than numerical computation for metrics. Therefore, there are no metric aggregations present in this query.'}</td>\n",
       "      <td>None</td>\n",
       "      <td>[instance_id, time_in_sec]</td>\n",
       "      <td>[application]</td>\n",
       "      <td>[instance_id, spawn_time]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>openstack</td>\n",
       "      <td>2</td>\n",
       "      <td>How long did it take to spawn instance 3edec1e4-9678-4a3a-a21b-a145a4ee5e61 on the hypervisor for openstack-tenant-2?</td>\n",
       "      <td>{application=\"openstack-tenant-2\", log_file_type=\"nova-compute\", component=\"nova.compute.manager\"} |= \"3edec1e4-9678-4a3a-a21b-a145a4ee5e61\" |= \"Took\" |= \"seconds to spawn the instance on the hypervisor\" | regexp \"\\\\[instance: (?P&lt;instance_id&gt;[^\\\\]]+)\\\\] Took (?P&lt;spawn_time&gt;\\\\d+\\\\.\\\\d+) seconds to spawn the instance on the hypervisor\" | line_format \"{{.instance_id}} took {{.spawn_time}}\"</td>\n",
       "      <td>bla</td>\n",
       "      <td>3edec1e4-9678-4a3a-a21b-a145a4ee5e61 took 20.58</td>\n",
       "      <td></td>\n",
       "      <td>{'chain_of_thought': 'Analyzing the query, it contains three label filters: `application=\"openstack\"`, `log_file_type=\"nova-compute\"`, `component=\"nova.compute.manager\"`. Additionally, there are three line filters (`|= \"3edec1e4-9678-4a3a-a21b-a145a4ee5e61\"`, `|= \"Took\"`, `|= \"seconds to spawn the instance on the hypervisor\"`) followed by a regex operation and a line format operation. The presence of three label filters classifies this under multiple log stream selectors, and the presence of more than one line filter classifies it under multiple line filters.', 'label_filter': 'multiple log stream selectors', 'line_filter': 'multiple line filters'}</td>\n",
       "      <td>multiple line filters</td>\n",
       "      <td>multiple log stream selectors</td>\n",
       "      <td>{'categories': None, 'chain_of_thought': 'The provided query involves log matching and extraction using filters and regexp but does not perform any quantitative aggregations or statistical computations, such as sum, count, or rate on the extracted data. It includes log filters, a regexp pattern for extracting data, and formatting the output using line_format, which suggests manipulation of text rather than numerical computation for metrics. Therefore, there are no metric aggregations present in this query.'}</td>\n",
       "      <td>None</td>\n",
       "      <td>[instance_id, time_in_sec]</td>\n",
       "      <td>[application]</td>\n",
       "      <td>[instance_id, spawn_time]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>openstack</td>\n",
       "      <td>3</td>\n",
       "      <td>What was the total time taken to build instance 3edec1e4-9678-4a3a-a21b-a145a4ee5e61 in openstack-us-east?</td>\n",
       "      <td>{application=\"openstack-us-east\", log_file_type=\"nova-compute\"} |= `3edec1e4-9678-4a3a-a21b-a145a4ee5e61` |= `Took` |= `seconds to build instance` | regexp `\\[instance: (?P&lt;instance_id&gt;[^\\]]+)\\] Took (?P&lt;build_time&gt;\\d+\\.\\d+) seconds to build instance` | line_format `{{.build_time}}`</td>\n",
       "      <td>1. {application=\"openstack\", log_file_type=\"nova-compute\"}\\nFetch all log lines matching label filters.\\n2. &lt;expr&gt; |= `3edec1e4-9678-4a3a-a21b-a145a4ee5e61`\\nReturn log lines that contain string 3edec1e4-9678-4a3a-a21b-a145a4ee5e61.\\n\\n3. &lt;expr&gt; |= `Took`\\nReturn log lines that contain string Took.\\n\\n4. &lt;expr&gt; |= `seconds to build instance`\\nReturn log lines that contain string seconds to build instance.\\n\\n5. &lt;expr&gt; | regexp `\\[instance: (?P&lt;instance_id&gt;[^\\]]+)\\] Took (?P&lt;build_time&gt;\\d+\\.\\d+) seconds to build instance`\\nThe regexp parser takes a single parameter | regexp \"&lt;re&gt;\" which is the regular expression using the Golang RE2 syntax. The regular expression must contain a least one named sub-match (e.g (?P&lt;name&gt;re)), each sub-match will extract a different label. The expression matches the structure of a log line. The extracted labels can be used in label filter expressions and used as values for a range aggregation via the unwrap operation.\\n\\n6. &lt;expr&gt; | line_format `{{.buil...</td>\n",
       "      <td>21.38</td>\n",
       "      <td></td>\n",
       "      <td>{'chain_of_thought': 'The user query submits using multiple label filters: `application='openstack'`, `log_file_type='nova-compute'`. There are multiple line filters used sequentially: `|= '3edec1e4-9678-4a3a-a21b-a145a4ee5e61'`, `|= 'Took'`, `|= 'seconds to build instance'`, `| regexp '\\[instance: (?P&lt;instance_id&gt;[^\\]]+)\\d+] Took (?P&lt;build_time&gt;\\d+.\\d+) seconds to build instance'`. By definition, using several different types of line filters suggests it falls under 'multiple line filters'. For labels, using multiple labels as part of the stream selector puts this into the 'multiple log stream selectors' category.', 'label_filter': 'multiple log stream selectors', 'line_filter': 'multiple line filters'}</td>\n",
       "      <td>multiple line filters</td>\n",
       "      <td>multiple log stream selectors</td>\n",
       "      <td>{'categories': None, 'chain_of_thought': 'This LogQL query does not contain any aggregation operators like `sum`, `avg`, `max`, `min`, `count`, etc. It appears to involve parsing and restructuring log lines with `regexp` and `line_format` but does not aggregate these logs into metrics. Therefore, it does not fall into the categories of metric aggregation, whether log range, unwrapped range, or built-in range aggregation.'}</td>\n",
       "      <td>None</td>\n",
       "      <td>[instance_id, time_in_sec]</td>\n",
       "      <td>[application]</td>\n",
       "      <td>[instance_id, build_time]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>openstack</td>\n",
       "      <td>3</td>\n",
       "      <td>What was the total time taken to build instance 3edec1e4-9678-4a3a-a21b-a145a4ee5e61 in openstack-tenant-1?</td>\n",
       "      <td>{application=\"openstack-tenant-1\", log_file_type=\"nova-compute\"} |= `3edec1e4-9678-4a3a-a21b-a145a4ee5e61` |= `Took` |= `seconds to build instance` | regexp `\\[instance: (?P&lt;instance_id&gt;[^\\]]+)\\] Took (?P&lt;build_time&gt;\\d+\\.\\d+) seconds to build instance` | line_format `{{.build_time}}`</td>\n",
       "      <td>1. {application=\"openstack\", log_file_type=\"nova-compute\"}\\nFetch all log lines matching label filters.\\n2. &lt;expr&gt; |= `3edec1e4-9678-4a3a-a21b-a145a4ee5e61`\\nReturn log lines that contain string 3edec1e4-9678-4a3a-a21b-a145a4ee5e61.\\n\\n3. &lt;expr&gt; |= `Took`\\nReturn log lines that contain string Took.\\n\\n4. &lt;expr&gt; |= `seconds to build instance`\\nReturn log lines that contain string seconds to build instance.\\n\\n5. &lt;expr&gt; | regexp `\\[instance: (?P&lt;instance_id&gt;[^\\]]+)\\] Took (?P&lt;build_time&gt;\\d+\\.\\d+) seconds to build instance`\\nThe regexp parser takes a single parameter | regexp \"&lt;re&gt;\" which is the regular expression using the Golang RE2 syntax. The regular expression must contain a least one named sub-match (e.g (?P&lt;name&gt;re)), each sub-match will extract a different label. The expression matches the structure of a log line. The extracted labels can be used in label filter expressions and used as values for a range aggregation via the unwrap operation.\\n\\n6. &lt;expr&gt; | line_format `{{.buil...</td>\n",
       "      <td>21.38</td>\n",
       "      <td></td>\n",
       "      <td>{'chain_of_thought': 'The user query submits using multiple label filters: `application='openstack'`, `log_file_type='nova-compute'`. There are multiple line filters used sequentially: `|= '3edec1e4-9678-4a3a-a21b-a145a4ee5e61'`, `|= 'Took'`, `|= 'seconds to build instance'`, `| regexp '\\[instance: (?P&lt;instance_id&gt;[^\\]]+)\\d+] Took (?P&lt;build_time&gt;\\d+.\\d+) seconds to build instance'`. By definition, using several different types of line filters suggests it falls under 'multiple line filters'. For labels, using multiple labels as part of the stream selector puts this into the 'multiple log stream selectors' category.', 'label_filter': 'multiple log stream selectors', 'line_filter': 'multiple line filters'}</td>\n",
       "      <td>multiple line filters</td>\n",
       "      <td>multiple log stream selectors</td>\n",
       "      <td>{'categories': None, 'chain_of_thought': 'This LogQL query does not contain any aggregation operators like `sum`, `avg`, `max`, `min`, `count`, etc. It appears to involve parsing and restructuring log lines with `regexp` and `line_format` but does not aggregate these logs into metrics. Therefore, it does not fall into the categories of metric aggregation, whether log range, unwrapped range, or built-in range aggregation.'}</td>\n",
       "      <td>None</td>\n",
       "      <td>[instance_id, time_in_sec]</td>\n",
       "      <td>[application]</td>\n",
       "      <td>[instance_id, build_time]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>hdfs</td>\n",
       "      <td>89</td>\n",
       "      <td>How many times did the PendingReplicationMonitor time out for blocks in the past 12 hours for hdfs-tenant-1?</td>\n",
       "      <td>sum(\\n  count_over_time(\\n    {application=\"hdfs-tenant-1\"}\\n    |~ \"PendingReplicationMonitor timed out block .*\"\\n    [12h]\\n  )\\n)</td>\n",
       "      <td>Explanation of the query:\\n\\n1. `{application=\"hdfs\"}`: This selects all logs from the HDFS application, as we don't have a specific component for PendingReplicationMonitor.\\n\\n2. `|~ \"PendingReplicationMonitor timed out block .*\"`: This line filter matches log lines containing the PendingReplicationMonitor timeout event.\\n\\n3. `[12h]`: This specifies the 12-hour time range as requested in the question.\\n\\n4. `count_over_time(...)`: This counts the occurrences of the matched log lines over the specified time range.\\n\\n5. `sum(...)`: This sums up all the counts, giving us the total number of times the PendingReplicationMonitor timed out for blocks in the past 12 hours.\\n\\nThis query efficiently counts the number of PendingReplicationMonitor timeout events across all HDFS components in the last 12 hours. The result will be a single value representing the total count of these timeout events.</td>\n",
       "      <td>2</td>\n",
       "      <td>Performance Issues</td>\n",
       "      <td>{'chain_of_thought': 'This query includes a single label filter: `application=\"hdfs\"`. Additionally, it contains a single line filter `|~ \"PendingReplicationMonitor timed out block .*\"` used to match logs with a specific pattern. There are no multiple filters used.', 'label_filter': 'single log stream selector', 'line_filter': 'single line filter'}</td>\n",
       "      <td>single line filter</td>\n",
       "      <td>single log stream selector</td>\n",
       "      <td>{'categories': ['log_range_aggregation', 'built_in_range_aggregation'], 'chain_of_thought': 'The query uses the `sum()` function as well as the `count_over_time()` function over a logging range of 12 hours specified. According to the documentation, `count_over_time` is categorized as a log range aggregation as it counts log entries over a specified time range. `sum()` is a built-in aggregation operator, used here to aggregate the counts over all labels.'}</td>\n",
       "      <td>[log_range_aggregation, built_in_range_aggregation]</td>\n",
       "      <td>[time_in_hours]</td>\n",
       "      <td>[application]</td>\n",
       "      <td>[time_in_hours]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>hdfs</td>\n",
       "      <td>89</td>\n",
       "      <td>How many times did the PendingReplicationMonitor time out for blocks in the past 12 hours for hdfs-tenant-2?</td>\n",
       "      <td>sum(\\n  count_over_time(\\n    {application=\"hdfs-tenant-2\"}\\n    |~ \"PendingReplicationMonitor timed out block .*\"\\n    [12h]\\n  )\\n)</td>\n",
       "      <td>Explanation of the query:\\n\\n1. `{application=\"hdfs\"}`: This selects all logs from the HDFS application, as we don't have a specific component for PendingReplicationMonitor.\\n\\n2. `|~ \"PendingReplicationMonitor timed out block .*\"`: This line filter matches log lines containing the PendingReplicationMonitor timeout event.\\n\\n3. `[12h]`: This specifies the 12-hour time range as requested in the question.\\n\\n4. `count_over_time(...)`: This counts the occurrences of the matched log lines over the specified time range.\\n\\n5. `sum(...)`: This sums up all the counts, giving us the total number of times the PendingReplicationMonitor timed out for blocks in the past 12 hours.\\n\\nThis query efficiently counts the number of PendingReplicationMonitor timeout events across all HDFS components in the last 12 hours. The result will be a single value representing the total count of these timeout events.</td>\n",
       "      <td>2</td>\n",
       "      <td>Performance Issues</td>\n",
       "      <td>{'chain_of_thought': 'This query includes a single label filter: `application=\"hdfs\"`. Additionally, it contains a single line filter `|~ \"PendingReplicationMonitor timed out block .*\"` used to match logs with a specific pattern. There are no multiple filters used.', 'label_filter': 'single log stream selector', 'line_filter': 'single line filter'}</td>\n",
       "      <td>single line filter</td>\n",
       "      <td>single log stream selector</td>\n",
       "      <td>{'categories': ['log_range_aggregation', 'built_in_range_aggregation'], 'chain_of_thought': 'The query uses the `sum()` function as well as the `count_over_time()` function over a logging range of 12 hours specified. According to the documentation, `count_over_time` is categorized as a log range aggregation as it counts log entries over a specified time range. `sum()` is a built-in aggregation operator, used here to aggregate the counts over all labels.'}</td>\n",
       "      <td>[log_range_aggregation, built_in_range_aggregation]</td>\n",
       "      <td>[time_in_hours]</td>\n",
       "      <td>[application]</td>\n",
       "      <td>[time_in_hours]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>hdfs</td>\n",
       "      <td>90</td>\n",
       "      <td>What is the average time taken for a block to be transmitted between DataNodes in the last hour for hdfs-us-east?</td>\n",
       "      <td>(\\n  sum(rate({application=\"hdfs-us-east\"} |~ \"Transmitted block\" [1h])) /\\n  sum(rate({application=\"hdfs-us-east\"} |~ \"Starting thread to transfer block\" [1h]))\\n) * 3600</td>\n",
       "      <td>Explanation of the query:\\n\\n1. `{application=\"hdfs-us-east\"}`: This selects all logs from  HDFS application.\\n\\n2. `|~ \"Transmitted block\"` and `|~ \"Starting thread to transfer block\"`: These line filters match log lines containing the end and start of block transfer events, respectively.\\n\\n3. `[1h]`: This specifies the 1-hour time range as requested in the question.\\n\\n4. `rate(... [1h])`: This calculates the per-second rate of occurrences for each event over the last hour.\\n\\n5. `sum(...)`: This sums the rates across all DataNodes.\\n\\n6. The division `(...) / (...)` gives us the average time between start and end events.\\n\\n7. `* 3600`: This converts the result from seconds to hours.\\n\\nThis query approximates the average time taken for a block to be transmitted between DataNodes in the last hour. It does this by calculating the ratio of completed transmissions to started transmissions and then converting this to an average time in seconds.\\n\\nNote that this method assumes that...</td>\n",
       "      <td>38k\\n&lt;graph&gt;</td>\n",
       "      <td>Replication and Data Transfer</td>\n",
       "      <td>{'chain_of_thought': 'The query has a single label filter: `application=\"hdfs\"`. The query is checking rates over log lines that match two different line filters: `|~ \"Transmitted block\"` and `|~ \"Starting thread to transfer block\"`. Since there are two different line filters used in separate sub-queries, this qualifies as multiple line filters. Therefore, the labels are single, and the line filters are multiple.', 'label_filter': 'single log stream selector', 'line_filter': 'multiple line filters'}</td>\n",
       "      <td>multiple line filters</td>\n",
       "      <td>single log stream selector</td>\n",
       "      <td>{'categories': ['log_range_aggregation', 'built_in_range_aggregation'], 'chain_of_thought': 'In this query, `sum()` and `rate()` are used. From the LogQL documentation, `rate()` functions as a log range aggregation, calculating the rate of logs over a specified time period. The `sum()` function is a built-in aggregation operator used here to sum up the rates calculated. The entire expression calculates a rate over an hour and uses built-in aggregation operators to sum these rates. These sums are then combined in a mathematical expression.'}</td>\n",
       "      <td>[log_range_aggregation, built_in_range_aggregation]</td>\n",
       "      <td>[time_in_hours]</td>\n",
       "      <td>[application]</td>\n",
       "      <td>[time_in_hours]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>hdfs</td>\n",
       "      <td>90</td>\n",
       "      <td>What is the average time taken for a block to be transmitted between DataNodes in the last hour for hdfs-tenant-1?</td>\n",
       "      <td>(\\n  sum(rate({application=\"hdfs-tenant-1\"} |~ \"Transmitted block\" [1h])) /\\n  sum(rate({application=\"hdfs-tenant-1\"} |~ \"Starting thread to transfer block\" [1h]))\\n) * 3600</td>\n",
       "      <td>Explanation of the query:\\n\\n1. `{application=\"hdfs-tenant-1\"}`: This selects all logs from  HDFS application.\\n\\n2. `|~ \"Transmitted block\"` and `|~ \"Starting thread to transfer block\"`: These line filters match log lines containing the end and start of block transfer events, respectively.\\n\\n3. `[1h]`: This specifies the 1-hour time range as requested in the question.\\n\\n4. `rate(... [1h])`: This calculates the per-second rate of occurrences for each event over the last hour.\\n\\n5. `sum(...)`: This sums the rates across all DataNodes.\\n\\n6. The division `(...) / (...)` gives us the average time between start and end events.\\n\\n7. `* 3600`: This converts the result from seconds to hours.\\n\\nThis query approximates the average time taken for a block to be transmitted between DataNodes in the last hour. It does this by calculating the ratio of completed transmissions to started transmissions and then converting this to an average time in seconds.\\n\\nNote that this method assumes tha...</td>\n",
       "      <td>38k\\n&lt;graph&gt;</td>\n",
       "      <td>Replication and Data Transfer</td>\n",
       "      <td>{'chain_of_thought': 'The query has a single label filter: `application=\"hdfs\"`. The query is checking rates over log lines that match two different line filters: `|~ \"Transmitted block\"` and `|~ \"Starting thread to transfer block\"`. Since there are two different line filters used in separate sub-queries, this qualifies as multiple line filters. Therefore, the labels are single, and the line filters are multiple.', 'label_filter': 'single log stream selector', 'line_filter': 'multiple line filters'}</td>\n",
       "      <td>multiple line filters</td>\n",
       "      <td>single log stream selector</td>\n",
       "      <td>{'categories': ['log_range_aggregation', 'built_in_range_aggregation'], 'chain_of_thought': 'In this query, `sum()` and `rate()` are used. From the LogQL documentation, `rate()` functions as a log range aggregation, calculating the rate of logs over a specified time period. The `sum()` function is a built-in aggregation operator used here to sum up the rates calculated. The entire expression calculates a rate over an hour and uses built-in aggregation operators to sum these rates. These sums are then combined in a mathematical expression.'}</td>\n",
       "      <td>[log_range_aggregation, built_in_range_aggregation]</td>\n",
       "      <td>[time_in_hours]</td>\n",
       "      <td>[application]</td>\n",
       "      <td>[time_in_hours]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>hdfs</td>\n",
       "      <td>90</td>\n",
       "      <td>What is the average time taken for a block to be transmitted between DataNodes in the last hour for hdfs-instance-2?</td>\n",
       "      <td>(\\n  sum(rate({application=\"hdfs-instance-2\"} |~ \"Transmitted block\" [1h])) /\\n  sum(rate({application=\"hdfs-instance-2\"} |~ \"Starting thread to transfer block\" [1h]))\\n) * 3600</td>\n",
       "      <td>Explanation of the query:\\n\\n1. `{application=\"hdfs-instance-2\"}`: This selects all logs from  HDFS application.\\n\\n2. `|~ \"Transmitted block\"` and `|~ \"Starting thread to transfer block\"`: These line filters match log lines containing the end and start of block transfer events, respectively.\\n\\n3. `[1h]`: This specifies the 1-hour time range as requested in the question.\\n\\n4. `rate(... [1h])`: This calculates the per-second rate of occurrences for each event over the last hour.\\n\\n5. `sum(...)`: This sums the rates across all DataNodes.\\n\\n6. The division `(...) / (...)` gives us the average time between start and end events.\\n\\n7. `* 3600`: This converts the result from seconds to hours.\\n\\nThis query approximates the average time taken for a block to be transmitted between DataNodes in the last hour. It does this by calculating the ratio of completed transmissions to started transmissions and then converting this to an average time in seconds.\\n\\nNote that this method assumes t...</td>\n",
       "      <td>38k\\n&lt;graph&gt;</td>\n",
       "      <td>Replication and Data Transfer</td>\n",
       "      <td>{'chain_of_thought': 'The query has a single label filter: `application=\"hdfs\"`. The query is checking rates over log lines that match two different line filters: `|~ \"Transmitted block\"` and `|~ \"Starting thread to transfer block\"`. Since there are two different line filters used in separate sub-queries, this qualifies as multiple line filters. Therefore, the labels are single, and the line filters are multiple.', 'label_filter': 'single log stream selector', 'line_filter': 'multiple line filters'}</td>\n",
       "      <td>multiple line filters</td>\n",
       "      <td>single log stream selector</td>\n",
       "      <td>{'categories': ['log_range_aggregation', 'built_in_range_aggregation'], 'chain_of_thought': 'In this query, `sum()` and `rate()` are used. From the LogQL documentation, `rate()` functions as a log range aggregation, calculating the rate of logs over a specified time period. The `sum()` function is a built-in aggregation operator used here to sum up the rates calculated. The entire expression calculates a rate over an hour and uses built-in aggregation operators to sum these rates. These sums are then combined in a mathematical expression.'}</td>\n",
       "      <td>[log_range_aggregation, built_in_range_aggregation]</td>\n",
       "      <td>[time_in_hours]</td>\n",
       "      <td>[application]</td>\n",
       "      <td>[time_in_hours]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>300 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    application  id  \\\n",
       "0     openstack   2   \n",
       "1     openstack   2   \n",
       "2     openstack   2   \n",
       "3     openstack   3   \n",
       "4     openstack   3   \n",
       "..          ...  ..   \n",
       "295        hdfs  89   \n",
       "296        hdfs  89   \n",
       "297        hdfs  90   \n",
       "298        hdfs  90   \n",
       "299        hdfs  90   \n",
       "\n",
       "                                                                                                                  question  \\\n",
       "0     How long did it take to spawn instance 3edec1e4-9678-4a3a-a21b-a145a4ee5e61 on the hypervisor for openstack-us-east?   \n",
       "1    How long did it take to spawn instance 3edec1e4-9678-4a3a-a21b-a145a4ee5e61 on the hypervisor for openstack-tenant-1?   \n",
       "2    How long did it take to spawn instance 3edec1e4-9678-4a3a-a21b-a145a4ee5e61 on the hypervisor for openstack-tenant-2?   \n",
       "3               What was the total time taken to build instance 3edec1e4-9678-4a3a-a21b-a145a4ee5e61 in openstack-us-east?   \n",
       "4              What was the total time taken to build instance 3edec1e4-9678-4a3a-a21b-a145a4ee5e61 in openstack-tenant-1?   \n",
       "..                                                                                                                     ...   \n",
       "295           How many times did the PendingReplicationMonitor time out for blocks in the past 12 hours for hdfs-tenant-1?   \n",
       "296           How many times did the PendingReplicationMonitor time out for blocks in the past 12 hours for hdfs-tenant-2?   \n",
       "297      What is the average time taken for a block to be transmitted between DataNodes in the last hour for hdfs-us-east?   \n",
       "298     What is the average time taken for a block to be transmitted between DataNodes in the last hour for hdfs-tenant-1?   \n",
       "299   What is the average time taken for a block to be transmitted between DataNodes in the last hour for hdfs-instance-2?   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                logql_query  \\\n",
       "0     {application=\"openstack-us-east\", log_file_type=\"nova-compute\", component=\"nova.compute.manager\"} |= \"3edec1e4-9678-4a3a-a21b-a145a4ee5e61\" |= \"Took\" |= \"seconds to spawn the instance on the hypervisor\" | regexp \"\\\\[instance: (?P<instance_id>[^\\\\]]+)\\\\] Took (?P<spawn_time>\\\\d+\\\\.\\\\d+) seconds to spawn the instance on the hypervisor\" | line_format \"{{.instance_id}} took {{.spawn_time}}\"   \n",
       "1    {application=\"openstack-tenant-1\", log_file_type=\"nova-compute\", component=\"nova.compute.manager\"} |= \"3edec1e4-9678-4a3a-a21b-a145a4ee5e61\" |= \"Took\" |= \"seconds to spawn the instance on the hypervisor\" | regexp \"\\\\[instance: (?P<instance_id>[^\\\\]]+)\\\\] Took (?P<spawn_time>\\\\d+\\\\.\\\\d+) seconds to spawn the instance on the hypervisor\" | line_format \"{{.instance_id}} took {{.spawn_time}}\"   \n",
       "2    {application=\"openstack-tenant-2\", log_file_type=\"nova-compute\", component=\"nova.compute.manager\"} |= \"3edec1e4-9678-4a3a-a21b-a145a4ee5e61\" |= \"Took\" |= \"seconds to spawn the instance on the hypervisor\" | regexp \"\\\\[instance: (?P<instance_id>[^\\\\]]+)\\\\] Took (?P<spawn_time>\\\\d+\\\\.\\\\d+) seconds to spawn the instance on the hypervisor\" | line_format \"{{.instance_id}} took {{.spawn_time}}\"   \n",
       "3                                                                                                               {application=\"openstack-us-east\", log_file_type=\"nova-compute\"} |= `3edec1e4-9678-4a3a-a21b-a145a4ee5e61` |= `Took` |= `seconds to build instance` | regexp `\\[instance: (?P<instance_id>[^\\]]+)\\] Took (?P<build_time>\\d+\\.\\d+) seconds to build instance` | line_format `{{.build_time}}`   \n",
       "4                                                                                                              {application=\"openstack-tenant-1\", log_file_type=\"nova-compute\"} |= `3edec1e4-9678-4a3a-a21b-a145a4ee5e61` |= `Took` |= `seconds to build instance` | regexp `\\[instance: (?P<instance_id>[^\\]]+)\\] Took (?P<build_time>\\d+\\.\\d+) seconds to build instance` | line_format `{{.build_time}}`   \n",
       "..                                                                                                                                                                                                                                                                                                                                                                                                      ...   \n",
       "295                                                                                                                                                                                                                                                                   sum(\\n  count_over_time(\\n    {application=\"hdfs-tenant-1\"}\\n    |~ \"PendingReplicationMonitor timed out block .*\"\\n    [12h]\\n  )\\n)   \n",
       "296                                                                                                                                                                                                                                                                   sum(\\n  count_over_time(\\n    {application=\"hdfs-tenant-2\"}\\n    |~ \"PendingReplicationMonitor timed out block .*\"\\n    [12h]\\n  )\\n)   \n",
       "297                                                                                                                                                                                                                             (\\n  sum(rate({application=\"hdfs-us-east\"} |~ \"Transmitted block\" [1h])) /\\n  sum(rate({application=\"hdfs-us-east\"} |~ \"Starting thread to transfer block\" [1h]))\\n) * 3600   \n",
       "298                                                                                                                                                                                                                           (\\n  sum(rate({application=\"hdfs-tenant-1\"} |~ \"Transmitted block\" [1h])) /\\n  sum(rate({application=\"hdfs-tenant-1\"} |~ \"Starting thread to transfer block\" [1h]))\\n) * 3600   \n",
       "299                                                                                                                                                                                                                       (\\n  sum(rate({application=\"hdfs-instance-2\"} |~ \"Transmitted block\" [1h])) /\\n  sum(rate({application=\"hdfs-instance-2\"} |~ \"Starting thread to transfer block\" [1h]))\\n) * 3600   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           query_explanation  \\\n",
       "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        bla   \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        bla   \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        bla   \n",
       "3    1. {application=\"openstack\", log_file_type=\"nova-compute\"}\\nFetch all log lines matching label filters.\\n2. <expr> |= `3edec1e4-9678-4a3a-a21b-a145a4ee5e61`\\nReturn log lines that contain string 3edec1e4-9678-4a3a-a21b-a145a4ee5e61.\\n\\n3. <expr> |= `Took`\\nReturn log lines that contain string Took.\\n\\n4. <expr> |= `seconds to build instance`\\nReturn log lines that contain string seconds to build instance.\\n\\n5. <expr> | regexp `\\[instance: (?P<instance_id>[^\\]]+)\\] Took (?P<build_time>\\d+\\.\\d+) seconds to build instance`\\nThe regexp parser takes a single parameter | regexp \"<re>\" which is the regular expression using the Golang RE2 syntax. The regular expression must contain a least one named sub-match (e.g (?P<name>re)), each sub-match will extract a different label. The expression matches the structure of a log line. The extracted labels can be used in label filter expressions and used as values for a range aggregation via the unwrap operation.\\n\\n6. <expr> | line_format `{{.buil...   \n",
       "4    1. {application=\"openstack\", log_file_type=\"nova-compute\"}\\nFetch all log lines matching label filters.\\n2. <expr> |= `3edec1e4-9678-4a3a-a21b-a145a4ee5e61`\\nReturn log lines that contain string 3edec1e4-9678-4a3a-a21b-a145a4ee5e61.\\n\\n3. <expr> |= `Took`\\nReturn log lines that contain string Took.\\n\\n4. <expr> |= `seconds to build instance`\\nReturn log lines that contain string seconds to build instance.\\n\\n5. <expr> | regexp `\\[instance: (?P<instance_id>[^\\]]+)\\] Took (?P<build_time>\\d+\\.\\d+) seconds to build instance`\\nThe regexp parser takes a single parameter | regexp \"<re>\" which is the regular expression using the Golang RE2 syntax. The regular expression must contain a least one named sub-match (e.g (?P<name>re)), each sub-match will extract a different label. The expression matches the structure of a log line. The extracted labels can be used in label filter expressions and used as values for a range aggregation via the unwrap operation.\\n\\n6. <expr> | line_format `{{.buil...   \n",
       "..                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       ...   \n",
       "295                                                                                                    Explanation of the query:\\n\\n1. `{application=\"hdfs\"}`: This selects all logs from the HDFS application, as we don't have a specific component for PendingReplicationMonitor.\\n\\n2. `|~ \"PendingReplicationMonitor timed out block .*\"`: This line filter matches log lines containing the PendingReplicationMonitor timeout event.\\n\\n3. `[12h]`: This specifies the 12-hour time range as requested in the question.\\n\\n4. `count_over_time(...)`: This counts the occurrences of the matched log lines over the specified time range.\\n\\n5. `sum(...)`: This sums up all the counts, giving us the total number of times the PendingReplicationMonitor timed out for blocks in the past 12 hours.\\n\\nThis query efficiently counts the number of PendingReplicationMonitor timeout events across all HDFS components in the last 12 hours. The result will be a single value representing the total count of these timeout events.   \n",
       "296                                                                                                    Explanation of the query:\\n\\n1. `{application=\"hdfs\"}`: This selects all logs from the HDFS application, as we don't have a specific component for PendingReplicationMonitor.\\n\\n2. `|~ \"PendingReplicationMonitor timed out block .*\"`: This line filter matches log lines containing the PendingReplicationMonitor timeout event.\\n\\n3. `[12h]`: This specifies the 12-hour time range as requested in the question.\\n\\n4. `count_over_time(...)`: This counts the occurrences of the matched log lines over the specified time range.\\n\\n5. `sum(...)`: This sums up all the counts, giving us the total number of times the PendingReplicationMonitor timed out for blocks in the past 12 hours.\\n\\nThis query efficiently counts the number of PendingReplicationMonitor timeout events across all HDFS components in the last 12 hours. The result will be a single value representing the total count of these timeout events.   \n",
       "297  Explanation of the query:\\n\\n1. `{application=\"hdfs-us-east\"}`: This selects all logs from  HDFS application.\\n\\n2. `|~ \"Transmitted block\"` and `|~ \"Starting thread to transfer block\"`: These line filters match log lines containing the end and start of block transfer events, respectively.\\n\\n3. `[1h]`: This specifies the 1-hour time range as requested in the question.\\n\\n4. `rate(... [1h])`: This calculates the per-second rate of occurrences for each event over the last hour.\\n\\n5. `sum(...)`: This sums the rates across all DataNodes.\\n\\n6. The division `(...) / (...)` gives us the average time between start and end events.\\n\\n7. `* 3600`: This converts the result from seconds to hours.\\n\\nThis query approximates the average time taken for a block to be transmitted between DataNodes in the last hour. It does this by calculating the ratio of completed transmissions to started transmissions and then converting this to an average time in seconds.\\n\\nNote that this method assumes that...   \n",
       "298  Explanation of the query:\\n\\n1. `{application=\"hdfs-tenant-1\"}`: This selects all logs from  HDFS application.\\n\\n2. `|~ \"Transmitted block\"` and `|~ \"Starting thread to transfer block\"`: These line filters match log lines containing the end and start of block transfer events, respectively.\\n\\n3. `[1h]`: This specifies the 1-hour time range as requested in the question.\\n\\n4. `rate(... [1h])`: This calculates the per-second rate of occurrences for each event over the last hour.\\n\\n5. `sum(...)`: This sums the rates across all DataNodes.\\n\\n6. The division `(...) / (...)` gives us the average time between start and end events.\\n\\n7. `* 3600`: This converts the result from seconds to hours.\\n\\nThis query approximates the average time taken for a block to be transmitted between DataNodes in the last hour. It does this by calculating the ratio of completed transmissions to started transmissions and then converting this to an average time in seconds.\\n\\nNote that this method assumes tha...   \n",
       "299  Explanation of the query:\\n\\n1. `{application=\"hdfs-instance-2\"}`: This selects all logs from  HDFS application.\\n\\n2. `|~ \"Transmitted block\"` and `|~ \"Starting thread to transfer block\"`: These line filters match log lines containing the end and start of block transfer events, respectively.\\n\\n3. `[1h]`: This specifies the 1-hour time range as requested in the question.\\n\\n4. `rate(... [1h])`: This calculates the per-second rate of occurrences for each event over the last hour.\\n\\n5. `sum(...)`: This sums the rates across all DataNodes.\\n\\n6. The division `(...) / (...)` gives us the average time between start and end events.\\n\\n7. `* 3600`: This converts the result from seconds to hours.\\n\\nThis query approximates the average time taken for a block to be transmitted between DataNodes in the last hour. It does this by calculating the ratio of completed transmissions to started transmissions and then converting this to an average time in seconds.\\n\\nNote that this method assumes t...   \n",
       "\n",
       "                                        query_result  \\\n",
       "0    3edec1e4-9678-4a3a-a21b-a145a4ee5e61 took 20.58   \n",
       "1    3edec1e4-9678-4a3a-a21b-a145a4ee5e61 took 20.58   \n",
       "2    3edec1e4-9678-4a3a-a21b-a145a4ee5e61 took 20.58   \n",
       "3                                              21.38   \n",
       "4                                              21.38   \n",
       "..                                               ...   \n",
       "295                                                2   \n",
       "296                                                2   \n",
       "297                                     38k\\n<graph>   \n",
       "298                                     38k\\n<graph>   \n",
       "299                                     38k\\n<graph>   \n",
       "\n",
       "                          category  \\\n",
       "0                                    \n",
       "1                                    \n",
       "2                                    \n",
       "3                                    \n",
       "4                                    \n",
       "..                             ...   \n",
       "295             Performance Issues   \n",
       "296             Performance Issues   \n",
       "297  Replication and Data Transfer   \n",
       "298  Replication and Data Transfer   \n",
       "299  Replication and Data Transfer   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          log_category_result  \\\n",
       "0                                                            {'chain_of_thought': 'Analyzing the query, it contains three label filters: `application=\"openstack\"`, `log_file_type=\"nova-compute\"`, `component=\"nova.compute.manager\"`. Additionally, there are three line filters (`|= \"3edec1e4-9678-4a3a-a21b-a145a4ee5e61\"`, `|= \"Took\"`, `|= \"seconds to spawn the instance on the hypervisor\"`) followed by a regex operation and a line format operation. The presence of three label filters classifies this under multiple log stream selectors, and the presence of more than one line filter classifies it under multiple line filters.', 'label_filter': 'multiple log stream selectors', 'line_filter': 'multiple line filters'}   \n",
       "1                                                            {'chain_of_thought': 'Analyzing the query, it contains three label filters: `application=\"openstack\"`, `log_file_type=\"nova-compute\"`, `component=\"nova.compute.manager\"`. Additionally, there are three line filters (`|= \"3edec1e4-9678-4a3a-a21b-a145a4ee5e61\"`, `|= \"Took\"`, `|= \"seconds to spawn the instance on the hypervisor\"`) followed by a regex operation and a line format operation. The presence of three label filters classifies this under multiple log stream selectors, and the presence of more than one line filter classifies it under multiple line filters.', 'label_filter': 'multiple log stream selectors', 'line_filter': 'multiple line filters'}   \n",
       "2                                                            {'chain_of_thought': 'Analyzing the query, it contains three label filters: `application=\"openstack\"`, `log_file_type=\"nova-compute\"`, `component=\"nova.compute.manager\"`. Additionally, there are three line filters (`|= \"3edec1e4-9678-4a3a-a21b-a145a4ee5e61\"`, `|= \"Took\"`, `|= \"seconds to spawn the instance on the hypervisor\"`) followed by a regex operation and a line format operation. The presence of three label filters classifies this under multiple log stream selectors, and the presence of more than one line filter classifies it under multiple line filters.', 'label_filter': 'multiple log stream selectors', 'line_filter': 'multiple line filters'}   \n",
       "3    {'chain_of_thought': 'The user query submits using multiple label filters: `application='openstack'`, `log_file_type='nova-compute'`. There are multiple line filters used sequentially: `|= '3edec1e4-9678-4a3a-a21b-a145a4ee5e61'`, `|= 'Took'`, `|= 'seconds to build instance'`, `| regexp '\\[instance: (?P<instance_id>[^\\]]+)\\d+] Took (?P<build_time>\\d+.\\d+) seconds to build instance'`. By definition, using several different types of line filters suggests it falls under 'multiple line filters'. For labels, using multiple labels as part of the stream selector puts this into the 'multiple log stream selectors' category.', 'label_filter': 'multiple log stream selectors', 'line_filter': 'multiple line filters'}   \n",
       "4    {'chain_of_thought': 'The user query submits using multiple label filters: `application='openstack'`, `log_file_type='nova-compute'`. There are multiple line filters used sequentially: `|= '3edec1e4-9678-4a3a-a21b-a145a4ee5e61'`, `|= 'Took'`, `|= 'seconds to build instance'`, `| regexp '\\[instance: (?P<instance_id>[^\\]]+)\\d+] Took (?P<build_time>\\d+.\\d+) seconds to build instance'`. By definition, using several different types of line filters suggests it falls under 'multiple line filters'. For labels, using multiple labels as part of the stream selector puts this into the 'multiple log stream selectors' category.', 'label_filter': 'multiple log stream selectors', 'line_filter': 'multiple line filters'}   \n",
       "..                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        ...   \n",
       "295                                                                                                                                                                                                                                                                                                                                                                            {'chain_of_thought': 'This query includes a single label filter: `application=\"hdfs\"`. Additionally, it contains a single line filter `|~ \"PendingReplicationMonitor timed out block .*\"` used to match logs with a specific pattern. There are no multiple filters used.', 'label_filter': 'single log stream selector', 'line_filter': 'single line filter'}   \n",
       "296                                                                                                                                                                                                                                                                                                                                                                            {'chain_of_thought': 'This query includes a single label filter: `application=\"hdfs\"`. Additionally, it contains a single line filter `|~ \"PendingReplicationMonitor timed out block .*\"` used to match logs with a specific pattern. There are no multiple filters used.', 'label_filter': 'single log stream selector', 'line_filter': 'single line filter'}   \n",
       "297                                                                                                                                                                                                                  {'chain_of_thought': 'The query has a single label filter: `application=\"hdfs\"`. The query is checking rates over log lines that match two different line filters: `|~ \"Transmitted block\"` and `|~ \"Starting thread to transfer block\"`. Since there are two different line filters used in separate sub-queries, this qualifies as multiple line filters. Therefore, the labels are single, and the line filters are multiple.', 'label_filter': 'single log stream selector', 'line_filter': 'multiple line filters'}   \n",
       "298                                                                                                                                                                                                                  {'chain_of_thought': 'The query has a single label filter: `application=\"hdfs\"`. The query is checking rates over log lines that match two different line filters: `|~ \"Transmitted block\"` and `|~ \"Starting thread to transfer block\"`. Since there are two different line filters used in separate sub-queries, this qualifies as multiple line filters. Therefore, the labels are single, and the line filters are multiple.', 'label_filter': 'single log stream selector', 'line_filter': 'multiple line filters'}   \n",
       "299                                                                                                                                                                                                                  {'chain_of_thought': 'The query has a single label filter: `application=\"hdfs\"`. The query is checking rates over log lines that match two different line filters: `|~ \"Transmitted block\"` and `|~ \"Starting thread to transfer block\"`. Since there are two different line filters used in separate sub-queries, this qualifies as multiple line filters. Therefore, the labels are single, and the line filters are multiple.', 'label_filter': 'single log stream selector', 'line_filter': 'multiple line filters'}   \n",
       "\n",
       "               line_filter                   label_filter  \\\n",
       "0    multiple line filters  multiple log stream selectors   \n",
       "1    multiple line filters  multiple log stream selectors   \n",
       "2    multiple line filters  multiple log stream selectors   \n",
       "3    multiple line filters  multiple log stream selectors   \n",
       "4    multiple line filters  multiple log stream selectors   \n",
       "..                     ...                            ...   \n",
       "295     single line filter     single log stream selector   \n",
       "296     single line filter     single log stream selector   \n",
       "297  multiple line filters     single log stream selector   \n",
       "298  multiple line filters     single log stream selector   \n",
       "299  multiple line filters     single log stream selector   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 metric_category_result  \\\n",
       "0                                     {'categories': None, 'chain_of_thought': 'The provided query involves log matching and extraction using filters and regexp but does not perform any quantitative aggregations or statistical computations, such as sum, count, or rate on the extracted data. It includes log filters, a regexp pattern for extracting data, and formatting the output using line_format, which suggests manipulation of text rather than numerical computation for metrics. Therefore, there are no metric aggregations present in this query.'}   \n",
       "1                                     {'categories': None, 'chain_of_thought': 'The provided query involves log matching and extraction using filters and regexp but does not perform any quantitative aggregations or statistical computations, such as sum, count, or rate on the extracted data. It includes log filters, a regexp pattern for extracting data, and formatting the output using line_format, which suggests manipulation of text rather than numerical computation for metrics. Therefore, there are no metric aggregations present in this query.'}   \n",
       "2                                     {'categories': None, 'chain_of_thought': 'The provided query involves log matching and extraction using filters and regexp but does not perform any quantitative aggregations or statistical computations, such as sum, count, or rate on the extracted data. It includes log filters, a regexp pattern for extracting data, and formatting the output using line_format, which suggests manipulation of text rather than numerical computation for metrics. Therefore, there are no metric aggregations present in this query.'}   \n",
       "3                                                                                                                            {'categories': None, 'chain_of_thought': 'This LogQL query does not contain any aggregation operators like `sum`, `avg`, `max`, `min`, `count`, etc. It appears to involve parsing and restructuring log lines with `regexp` and `line_format` but does not aggregate these logs into metrics. Therefore, it does not fall into the categories of metric aggregation, whether log range, unwrapped range, or built-in range aggregation.'}   \n",
       "4                                                                                                                            {'categories': None, 'chain_of_thought': 'This LogQL query does not contain any aggregation operators like `sum`, `avg`, `max`, `min`, `count`, etc. It appears to involve parsing and restructuring log lines with `regexp` and `line_format` but does not aggregate these logs into metrics. Therefore, it does not fall into the categories of metric aggregation, whether log range, unwrapped range, or built-in range aggregation.'}   \n",
       "..                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  ...   \n",
       "295                                                                                         {'categories': ['log_range_aggregation', 'built_in_range_aggregation'], 'chain_of_thought': 'The query uses the `sum()` function as well as the `count_over_time()` function over a logging range of 12 hours specified. According to the documentation, `count_over_time` is categorized as a log range aggregation as it counts log entries over a specified time range. `sum()` is a built-in aggregation operator, used here to aggregate the counts over all labels.'}   \n",
       "296                                                                                         {'categories': ['log_range_aggregation', 'built_in_range_aggregation'], 'chain_of_thought': 'The query uses the `sum()` function as well as the `count_over_time()` function over a logging range of 12 hours specified. According to the documentation, `count_over_time` is categorized as a log range aggregation as it counts log entries over a specified time range. `sum()` is a built-in aggregation operator, used here to aggregate the counts over all labels.'}   \n",
       "297  {'categories': ['log_range_aggregation', 'built_in_range_aggregation'], 'chain_of_thought': 'In this query, `sum()` and `rate()` are used. From the LogQL documentation, `rate()` functions as a log range aggregation, calculating the rate of logs over a specified time period. The `sum()` function is a built-in aggregation operator used here to sum up the rates calculated. The entire expression calculates a rate over an hour and uses built-in aggregation operators to sum these rates. These sums are then combined in a mathematical expression.'}   \n",
       "298  {'categories': ['log_range_aggregation', 'built_in_range_aggregation'], 'chain_of_thought': 'In this query, `sum()` and `rate()` are used. From the LogQL documentation, `rate()` functions as a log range aggregation, calculating the rate of logs over a specified time period. The `sum()` function is a built-in aggregation operator used here to sum up the rates calculated. The entire expression calculates a rate over an hour and uses built-in aggregation operators to sum these rates. These sums are then combined in a mathematical expression.'}   \n",
       "299  {'categories': ['log_range_aggregation', 'built_in_range_aggregation'], 'chain_of_thought': 'In this query, `sum()` and `rate()` are used. From the LogQL documentation, `rate()` functions as a log range aggregation, calculating the rate of logs over a specified time period. The `sum()` function is a built-in aggregation operator used here to sum up the rates calculated. The entire expression calculates a rate over an hour and uses built-in aggregation operators to sum these rates. These sums are then combined in a mathematical expression.'}   \n",
       "\n",
       "                                         metric_category  \\\n",
       "0                                                   None   \n",
       "1                                                   None   \n",
       "2                                                   None   \n",
       "3                                                   None   \n",
       "4                                                   None   \n",
       "..                                                   ...   \n",
       "295  [log_range_aggregation, built_in_range_aggregation]   \n",
       "296  [log_range_aggregation, built_in_range_aggregation]   \n",
       "297  [log_range_aggregation, built_in_range_aggregation]   \n",
       "298  [log_range_aggregation, built_in_range_aggregation]   \n",
       "299  [log_range_aggregation, built_in_range_aggregation]   \n",
       "\n",
       "                      variables application_variables  \\\n",
       "0    [instance_id, time_in_sec]         [application]   \n",
       "1    [instance_id, time_in_sec]         [application]   \n",
       "2    [instance_id, time_in_sec]         [application]   \n",
       "3    [instance_id, time_in_sec]         [application]   \n",
       "4    [instance_id, time_in_sec]         [application]   \n",
       "..                          ...                   ...   \n",
       "295             [time_in_hours]         [application]   \n",
       "296             [time_in_hours]         [application]   \n",
       "297             [time_in_hours]         [application]   \n",
       "298             [time_in_hours]         [application]   \n",
       "299             [time_in_hours]         [application]   \n",
       "\n",
       "                 row_variables  \n",
       "0    [instance_id, spawn_time]  \n",
       "1    [instance_id, spawn_time]  \n",
       "2    [instance_id, spawn_time]  \n",
       "3    [instance_id, build_time]  \n",
       "4    [instance_id, build_time]  \n",
       "..                         ...  \n",
       "295            [time_in_hours]  \n",
       "296            [time_in_hours]  \n",
       "297            [time_in_hours]  \n",
       "298            [time_in_hours]  \n",
       "299            [time_in_hours]  \n",
       "\n",
       "[300 rows x 15 columns]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expanded_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f181cd91f48446ca75fa9cf11387c63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "537d23531243488b8a11f06d58bb8326",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d19c01bf58042d3b69dcc24c9c298ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/955 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/sidbin/natural-logql/commit/762df8a43ad7e14762ab6d80a2e674c6bf6855f9', commit_message='Upload dataset', commit_description='', oid='762df8a43ad7e14762ab6d80a2e674c6bf6855f9', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/sidbin/natural-logql', endpoint='https://huggingface.co', repo_type='dataset', repo_id='sidbin/natural-logql'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expanded_dataset.push_to_hub(\"sidbin/natural-logql\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Row Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_vary = [\"question\", \"logql_query\", \"query_explanation\", \"query_result\"]\n",
    "columns_to_keep = [col for col in all_columns if col not in columns_to_vary]\n",
    "\n",
    "columns_for_llm = columns_to_vary\n",
    "columns_for_llm.append(\"row_variables\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_for_llm.append(\"application_variables\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['question', 'logql_query', 'query_explanation', 'query_result', 'row_variables']\n",
      "['application', 'id', 'category', 'log_category_result', 'line_filter', 'label_filter', 'metric_category_result', 'metric_category', 'variables', 'application_variables', 'row_variables']\n",
      "['question', 'logql_query', 'query_explanation', 'query_result', 'row_variables']\n"
     ]
    }
   ],
   "source": [
    "print(columns_to_vary)\n",
    "print(columns_to_keep)\n",
    "print(columns_for_llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>logql_query</th>\n",
       "      <th>query_explanation</th>\n",
       "      <th>query_result</th>\n",
       "      <th>row_variables</th>\n",
       "      <th>application_variables</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>How many times did the NameSystem allocate new blocks in the past minute?</td>\n",
       "      <td>sum(\\n    count_over_time(\\n        {application=\"hdfs\"}\\n        |~ \"BLOCK\\\\* NameSystem\\\\.allocateBlock:\"\\n        [1m]\\n    )\\n)</td>\n",
       "      <td>1. `{application=\"hdfs\"}`: This selects all logs from the FSNamesystem component, which handles block allocation.\\n\\n2. `|~ \"BLOCK\\\\* NameSystem\\\\.allocateBlock:\"`: This line filter matches log lines containing the block allocation event. We use `\\\\` to escape the asterisk in the log message.\\n\\n3. `[1h]`: This specifies the 1-minute time range as requested in the question.\\n\\n4. `count_over_time(...)`: This counts the occurrences of the matched log lines over the specified time range.\\n\\n5. `sum(...)`: This sums up all the counts, giving us the total number of block allocations across all instances of FSNamesystem.\\n\\nThis query efficiently counts the number of times the NameSystem allocated new blocks in the past hour. The result will be a single value representing the total count of block allocations.\\n</td>\n",
       "      <td>1.88k\\n&lt;graph&gt;</td>\n",
       "      <td>[time_in_minutes, block_action: addStoredBlock|delete]</td>\n",
       "      <td>[application]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                     question  \\\n",
       "94  How many times did the NameSystem allocate new blocks in the past minute?   \n",
       "\n",
       "                                                                                                                            logql_query  \\\n",
       "94  sum(\\n    count_over_time(\\n        {application=\"hdfs\"}\\n        |~ \"BLOCK\\\\* NameSystem\\\\.allocateBlock:\"\\n        [1m]\\n    )\\n)   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    query_explanation  \\\n",
       "94  1. `{application=\"hdfs\"}`: This selects all logs from the FSNamesystem component, which handles block allocation.\\n\\n2. `|~ \"BLOCK\\\\* NameSystem\\\\.allocateBlock:\"`: This line filter matches log lines containing the block allocation event. We use `\\\\` to escape the asterisk in the log message.\\n\\n3. `[1h]`: This specifies the 1-minute time range as requested in the question.\\n\\n4. `count_over_time(...)`: This counts the occurrences of the matched log lines over the specified time range.\\n\\n5. `sum(...)`: This sums up all the counts, giving us the total number of block allocations across all instances of FSNamesystem.\\n\\nThis query efficiently counts the number of times the NameSystem allocated new blocks in the past hour. The result will be a single value representing the total count of block allocations.\\n   \n",
       "\n",
       "      query_result                                           row_variables  \\\n",
       "94  1.88k\\n<graph>  [time_in_minutes, block_action: addStoredBlock|delete]   \n",
       "\n",
       "   application_variables  \n",
       "94         [application]  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[columns_for_llm].sample(n=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
